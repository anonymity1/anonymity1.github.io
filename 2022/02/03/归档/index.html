<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>归档 - Hexo</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Hexo"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Hexo"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="一些思考后给出答案，但是不成体系的问题。"><meta property="og:type" content="blog"><meta property="og:title" content="归档"><meta property="og:url" content="https://github.com/anonymity1/anonymity1.github.io/2022/02/03/%E5%BD%92%E6%A1%A3/"><meta property="og:site_name" content="Hexo"><meta property="og:description" content="一些思考后给出答案，但是不成体系的问题。"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://github.com/img/Cmake编译系统/1.png"><meta property="og:image" content="https://github.com/img/Gradient-Descent/func.png"><meta property="og:image" content="https://github.com/img/Gradient-Descent/loss.png"><meta property="og:image" content="https://github.com/img/Gradient-Descent/gd.png"><meta property="og:image" content="https://github.com/img/Metric-Space/coveringAndPacking.png"><meta property="article:published_time" content="2022-02-03T09:28:07.000Z"><meta property="article:modified_time" content="2024-08-07T12:41:46.546Z"><meta property="article:author" content="Hesheng Sun"><meta property="article:tag" content="瞎扯淡"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/Cmake编译系统/1.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://github.com/anonymity1/anonymity1.github.io/2022/02/03/%E5%BD%92%E6%A1%A3/"},"headline":"归档","image":["https://github.com/img/Cmake编译系统/1.png","https://github.com/img/Gradient-Descent/func.png","https://github.com/img/Gradient-Descent/loss.png","https://github.com/img/Gradient-Descent/gd.png","https://github.com/img/Metric-Space/coveringAndPacking.png"],"datePublished":"2022-02-03T09:28:07.000Z","dateModified":"2024-08-07T12:41:46.546Z","author":{"@type":"Person","name":"Hesheng Sun"},"publisher":{"@type":"Organization","name":"Hexo","logo":{"@type":"ImageObject","url":{"text":"anonymity1"}}},"description":"一些思考后给出答案，但是不成体系的问题。"}</script><link rel="canonical" href="https://github.com/anonymity1/anonymity1.github.io/2022/02/03/%E5%BD%92%E6%A1%A3/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.2.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">anonymity1</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/tags">Tags</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Github" href="https://github.com/anonymity1/anonymity1.github.io"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-02-03T09:28:07.000Z" title="2022/2/3 下午5:28:07">2022-02-03</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-08-07T12:41:46.546Z" title="2024/8/7 下午8:41:46">2024-08-07</time></span><span class="level-item">41 minutes read (About 6083 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">归档</h1><div class="content"><p>一些思考后给出答案，但是不成体系的问题。</p>
<a id="more"></a>
<h2 id="概率空间定义要用三个要素"><a href="#概率空间定义要用三个要素" class="headerlink" title="概率空间定义要用三个要素"></a>概率空间定义要用三个要素</h2><p>概率空间定义，样本空间、事件空间、概率测度</p>
<p>我们直觉上、习惯上是将概率定义在样本空间上，然而实际上严格定义是在事件空间上，事件空间是sigma代数，</p>
<p>这是为了解释当样本空间有不可数集的时候，如何定义概率的问题。</p>
<h2 id="泛函“只”有一阶导"><a href="#泛函“只”有一阶导" class="headerlink" title="泛函“只”有一阶导"></a>泛函“只”有一阶导</h2><p>泛函是函数向实数域的映射。</p>
<p>泛函取极值的必要条件是euler-lagrange方程成立。</p>
<p>我们看到的euler-lagrange方程是依赖于路径、路径导数和自变量的微分方程。</p>
<p>广义的euler-lagrange方程依赖路径的更高阶导数。</p>
<p>最速降线没有用广义euler-lagrange方程只是采用euler-lagrange方程的标准形就能解出来路径方程，并且和事实相符。</p>
<h2 id="动态规划是强化学习"><a href="#动态规划是强化学习" class="headerlink" title="动态规划是强化学习"></a>动态规划是强化学习</h2><p>动态规划是有模型强化学习。有模型强化学习不需要通过数据进行训练。动态规划的不同状态是指具有不一样的参数的同一个优化问题。</p>
<p>比如背包问题，背包容量不一样，物品数量不一样，就是一个状态。</p>
<p>形式化成一个优化问题后，发现可以通过max，min或者分段函数这样的非线性操作进行状态转化，转移函数就对应强化学习里的模型。</p>
<p>动态规划能解的问题很有限，不能有后效性，且初始子问题足够简单，才能求解。找转移函数的过程比较困难，需要积累。</p>
<p>背包dp，序列dp，树状dp，DAG-dp等。</p>
<h2 id="Caffe代码结构"><a href="#Caffe代码结构" class="headerlink" title="Caffe代码结构"></a>Caffe代码结构</h2><p>caffe组织推理的过程分为四层结构：syncedmem -&gt; blob -&gt; layer -&gt; net</p>
<p>syncedmem是最底层的线性数据结构，用来维护cpu和gpu的存储同步。blob是更高一层的抽象，维护张量的形状和数据，是所有张量操作的基本类型。layer是构成神经网络的基本单元，net就是最终的推理结构。</p>
<p>Caffe除了完成推理，还要执行训练，对异构设备的适应，以及对单独CPU执行的支持，因此Caffe对于内存进行syncedmem这一层的封装。我们只需要在前向推理时实现多流并发，syncedmem这一层封装可以不需要，可以在blob内部完成内存和显存的数据转化和分配。</p>
<h2 id="im2col对卷积的实现"><a href="#im2col对卷积的实现" class="headerlink" title="im2col对卷积的实现"></a>im2col对卷积的实现</h2><p>二维卷积的朴素实现包括7个循环：batch_size，输出通道数量、输入通道数量、卷积核高、卷积核宽、沿宽方向滑动距离、延高方向滑动距离。以此类推，三维卷积9个循环。</p>
<p>这么多循环放在CUDA中，不利于优化，且缓存不命中，频繁访存执行效率低。考虑到<strong>卷积核和输入张量对应点的相乘，与向量点乘的运算过程是一样的</strong>，而矩阵乘法是向量点乘的堆叠，因此可以设计一种先将输入转化成矩阵，然后执行矩阵的相乘加速卷积速度的方法。</p>
<p>方法改变，结果不改变，应保证转化+矩阵相乘与朴素实现的输出结果一样。</p>
<p>朴素卷积输出张量$O$的维度是</p>
<p>[batch_size, conv_out_channels, spatial_output_shape_h, spatial_output_shape_w]，</p>
<p>卷积层保存的权重$W$维度是</p>
<p>[conv_out_channels, conv_in_channels, kernel_h, kernel_w]。</p>
<p>卷积核的维度就是</p>
<p>[conv_in_channels, kernel_h, kernel_w]</p>
<p>设$O = W * D$，$D$是输入张量转化而来形成的矩阵。考虑到张量在内存和显存中均是一维存储，可以将$W$视作行数为conv_out_channels，列数为conv_in_channels * kernel_h * kernel_w的矩阵，$O$视作行数为batch_size * conv_out_channels，列数为spatial_h * spatial_w的矩阵。</p>
<p>转化得来的矩阵$D$的行数需要是$W$的列数，列数是$O$的列数，才能满足$O = W * D$的表达式。同时左矩阵的行向量与右矩阵的列向量点乘需要等价于卷积核和张量对应区域的相乘。</p>
<p>考虑batch_size中的输入张量独立，因此可将$O$划分成batch_size个张量，$D$的行数为conv_in_channels * kernel_h * kernel_w，列数为spatial_h * spatial_w，从而首先确定维度。其次，根据$O$中列号对应的输出张量空间位置，计算在<strong>原始输入张量的空间位置中</strong>的哪些元素与卷积核相乘，在输入张量中按行展开这些元素，按卷积核通道序号次序放入列号对应的列向量。</p>
<p>计算方式如下代码所示。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* compute the output_spatial_shape */</span></span><br><span class="line"><span class="keyword">int</span> height_col = (height + <span class="number">2</span> * pad_h - (dilation_h * (kernel_h - <span class="number">1</span>) + <span class="number">1</span>)) / stride_h + <span class="number">1</span>;</span><br><span class="line"><span class="keyword">int</span> width_col = (width + <span class="number">2</span> * pad_w - (dilation_w * (kernel_w - <span class="number">1</span>) + <span class="number">1</span>)) / stride_w + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">CUDA_KERNEL_LOOP(index, conv_in_channels * height_col * width_col) &#123;</span><br><span class="line">  <span class="comment">/* compute the first position in input spatial tensor */</span></span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">int</span> c_im = h_index / height_col / width_col; <span class="comment">// the number of conv_in_channels</span></span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">int</span> h_offset = h_col * stride_h - pad_h;</span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">int</span> w_offset = w_col * stride_w - pad_w;</span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">float</span>* data_im_ptr = data_im;</span><br><span class="line">  </span><br><span class="line">  data_im_ptr += (c_im * height + h_offset) * width + w_offset; <span class="comment">// the first position</span></span><br><span class="line">  <span class="comment">/* the corresponding position */</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; kernel_h; i++) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; kernel_w; j++) &#123;</span><br><span class="line">      <span class="keyword">int</span> h_im = h_offset + i * dilation_h;</span><br><span class="line">      <span class="keyword">int</span> w_im = w_offset + j * dilation_w;</span><br><span class="line">      *data_col_ptr = (h_im &gt;= <span class="number">0</span> &amp;&amp; w_im &gt;= <span class="number">0</span> &amp;&amp; h_im &lt; height &amp;&amp; w_im &lt; width) ?</span><br><span class="line">        data_im_ptr[i * dilation_h * width + j * dilation_w] : <span class="number">0</span>;</span><br><span class="line">      data_col_ptr += height_col * width_col; <span class="comment">// next row</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从输出张量反推输入张量位置是计算的关键步骤，对应即是h_offset、w_offset以及data_im_ptr的计算过程。卷积的整体前向传播过程即分为两步。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ConvolutionLayer::Forward</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;Blob*&gt;&amp; bottom, <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;Blob*&gt;&amp; top, cudaStream_t stream)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">float</span>* weight = <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>]-&gt;gpu_data();</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; bottom.size(); i++) &#123;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">float</span>* bottom_data = bottom[i]-&gt;gpu_data();</span><br><span class="line">    <span class="keyword">float</span>* top_data = top[i]-&gt;mutable_gpu_data();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> n = <span class="number">0</span>; n &lt; num_; n++) &#123;</span><br><span class="line">      <span class="comment">// bottom_data -&gt; col_buffer_</span></span><br><span class="line">      <span class="comment">// weight * col_buffer_ = top_data </span></span><br><span class="line">      im2col(bottom_data + n * bottom[<span class="number">0</span>]-&gt;count(channels_), channels_, </span><br><span class="line">        bottom[<span class="number">0</span>]-&gt;shape(channel_axis_ + <span class="number">1</span>), bottom[<span class="number">0</span>]-&gt;shape(channel_axis_ + <span class="number">2</span>),</span><br><span class="line">        kernel_h_, kernel_w_, pad_h_, pad_w_, stride_h_, stride_w_, dilation_h_, dilation_w_,</span><br><span class="line">        col_buffer_-&gt;mutable_gpu_data(), stream</span><br><span class="line">      );</span><br><span class="line">      acc_gemm(CUBLAS_OP_N, CUBLAS_OP_N, </span><br><span class="line">        conv_out_channels_, conv_out_spatial_count_, kernel_count_, </span><br><span class="line">        <span class="number">1.0</span>, weight, col_buffer_-&gt;mutable_gpu_data(), <span class="number">0.0</span>, top_data, stream</span><br><span class="line">      );</span><br><span class="line">      <span class="keyword">if</span> (bias_term_) &#123;</span><br><span class="line">        acc_gemm(CUBLAS_OP_N, CUBLAS_OP_N, </span><br><span class="line">          conv_out_channels_, conv_out_spatial_count_, <span class="number">1</span>,</span><br><span class="line">          <span class="number">1.0</span>, <span class="keyword">this</span>-&gt;blobs_[<span class="number">1</span>]-&gt;gpu_data(), bias_multiplier_-&gt;gpu_data(), </span><br><span class="line">          <span class="number">1.0</span>, top_data + n * conv_out_channels_ * kernel_count_, stream  </span><br><span class="line">        );</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="C语言宏的替换"><a href="#C语言宏的替换" class="headerlink" title="C语言宏的替换"></a>C语言宏的替换</h2><p>第一种，被替换对象是被标点或空格分割开的“独立字符串”，用#表示。<br>第二种，被替换对象被一个独立字符串真包含，用##表示，当被替换对象位于独立字符串开头，则在被替换对象的结尾加##表征；当被替换对象位于独立字符串结尾，则在被替换对象的头部加##表征；当被替换对象位于独立字符串中间，则在被替换对象的开头和结尾都加##表征。</p>
<p>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> REGISTER_LAYER_CREATOR(type, creator) \</span></span><br><span class="line">  LayerRegisterer g_creator#<span class="meta">#type(#type, creator); \</span></span><br></pre></td></tr></table></figure>
<h2 id="C语言静态库"><a href="#C语言静态库" class="headerlink" title="C语言静态库"></a>C语言静态库</h2><p>静态库方便移植，速度快，但是体积大，编译慢，这个都知道。除此之外，静态库中的static变量不会被事先声明，如果在头文件定义static变量，可能有重复定义的风险。</p>
<p>怎么解决呢？制作一个只有可能被应用程序引用的头文件，在其中定义需要预先定义的static变量。</p>
<p>例子：不同layer初始化注册map表。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* register_layer.h */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> REGISTER_LAYER_H</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> REGISTER_LAYER_H</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;u1/layer_factory.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;u1/layer/full_layer.h&quot;</span></span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">REGISTER_LAYER_CLASS(Full);</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure>
<h2 id="C-内存分配"><a href="#C-内存分配" class="headerlink" title="C++内存分配"></a>C++内存分配</h2><ol>
<li><p>C++为了避免手动free或者delete的问题引入std::shared_ptr&lt;&gt;，即智能指针。智能指针调用对象方法时和一般指针一样，变成一般指针时用.get()方法.</p>
</li>
<li><p>当具有独显时：</p>
</li>
</ol>
<ul>
<li><p>C++仅有分配内存的能力，显存的分配用CUDA接口；</p>
</li>
<li><p>gpu计算单元不能访问内存，cpu也不能访问显存，</p>
</li>
<li><p>显存和内存可以互通，但是显存分配不提供智能指针，仅提供原始类型的指针。</p>
</li>
</ul>
<h2 id="core-dump"><a href="#core-dump" class="headerlink" title="core dump"></a>core dump</h2><p>一般逻辑问题查找日志基本可以解决。Core Dump没有日志报错信息，或者日志容易出问题。这时需要用调试工具。</p>
<p>使用gdb+cmake编译时，添加-g编译选项，如下段代码。用ulimit -c nolimited取消core dump文件的大小限制。更改/proc/sys/kernel/core_pattern的core dump文件输出位置和命名，然后用gdb &lt;程序名&gt; &lt;修改后的coredump文件名&gt; 进入gdb调试。在gdb中用bt查找调用栈，一般能定位内存出问题的位置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CMakeLists.txt</span></span><br><span class="line">ADD_DEFINITIONS(-g -D_REENTRANT -D_FILE_OFFSET_BITS=<span class="number">64</span> -DAC_HAS_INFO</span><br><span class="line">        -DAC_HAS_WARNING -DAC_HAS_ERROR -DAC_HAS_CRITICAL -DTIXML_USE_STL</span><br><span class="line">        -DAC_HAS_DEBUG -DLINUX_DAEMON)</span><br></pre></td></tr></table></figure>
<h2 id="工厂模式的好处"><a href="#工厂模式的好处" class="headerlink" title="工厂模式的好处"></a>工厂模式的好处</h2><p>神经网络前向推理初始化的时候，每个层需要根据InferParameter中定义的layer编号类型定义并创建。不同类型的layer对应抽象layer的不同子类实现，如果没有工厂模式的话，需要在初始化layer的时候，根据layer的类型一一对应写出其构造函数，当类型很多时，这一步很繁琐。</p>
<p>用工厂模式，就是初始化的时候很简单，一行代码，工厂模式自己找到对应的构造函数，如下所示。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> layer_id = <span class="number">0</span>; layer_id &lt; para.layer_size(); layer_id++) &#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  layers_.push_back(LayerRegistry::CreateLayer(para.layer(layer_id)));</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数对应实现如下，在一个全局维护的map中，根据layer类型查找对应的初始化函数。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> LAYER_FACTORY_H</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> LAYER_FACTORY_H</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;u1/layer.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;u1/proto/test.pb.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerRegistry</span> &#123;</span></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  LayerRegistry() &#123; &#125;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="keyword">typedef</span> <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Layer&gt; (*Creator) (<span class="keyword">const</span> u1::LayerParameter&amp;);</span><br><span class="line">  <span class="keyword">typedef</span> <span class="built_in">std</span>::<span class="built_in">map</span>&lt;<span class="built_in">std</span>::<span class="built_in">string</span>, Creator&gt; CreatorRegistry;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// create CreatorRegistry ...</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">static</span> <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Layer&gt; <span class="title">CreateLayer</span><span class="params">(<span class="keyword">const</span> u1::LayerParameter&amp; para)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; type = para.type();</span><br><span class="line">    CreatorRegistry&amp; registry = Registry();</span><br><span class="line">    <span class="keyword">if</span> (registry.count(type) != <span class="number">1</span>) &#123;</span><br><span class="line">      <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Unknown layer type: &quot;</span> &lt;&lt; type &lt;&lt; <span class="string">&quot;(known types: &quot;</span> &lt;&lt; LayerTypeListString() &lt;&lt; <span class="string">&quot;)&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> registry[type](para);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure>
<p>除此之外，工厂模式中每个layer都有自己的特殊SetUp函数，如下面第5行代码所示，也不用根据type类型再写额外的判断逻辑。（也是C++重载的好处？指向基类的指针调用不同子类的重载函数？）<strong>总之，工厂模式，就是为了在使用“工厂”的产品时，不用加一堆分支判断逻辑。</strong>（其实就两字：方便！）</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> layer_id = <span class="number">0</span>; layer_id &lt; para.layer_size(); layer_id++) &#123;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  layers_.push_back(LayerRegistry::CreateLayer(para.layer(layer_id)));</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">  layers_[layer_id]-&gt;SetUp(bottom_vecs_[layer_id], top_vecs_[layer_id]);</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>那么工厂模式如何实现的呢？第一，多种layer实现时需要采用继承的方法；第二是维护一个全局的初始化函数表，这样在初始化时查找这个表就可以避免写判断逻辑，这个初始化表可以在类实现中直接注册。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> REGISTER_LAYER_CREATOR(type, creator) \</span></span><br><span class="line">  LayerRegisterer g_creator#<span class="meta">#type(#type, creator); \</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> REGISTER_LAYER_CLASS(type) \</span></span><br><span class="line">  <span class="keyword">static</span> <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Layer&gt; Creator_#<span class="meta">#type##Layer(const u1::LayerParameter&amp; para) &#123; \</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;Layer&gt;(<span class="keyword">new</span> type##Layer(para)); \</span><br><span class="line">  &#125; \</span><br><span class="line">  REGISTER_LAYER_CREATOR(type, Creator_#<span class="meta">#type##Layer) \</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure>
<p>比如这段代码中，LayerRegisterer的初始化调用了一个static方法实现creator的注册，这两个宏实现了这个类的初始化，所以我们看到只需要一行代码就可以实现不同layer子类的注册。</p>
<h2 id="CUBLAS加速矩阵乘法的一个小问题"><a href="#CUBLAS加速矩阵乘法的一个小问题" class="headerlink" title="CUBLAS加速矩阵乘法的一个小问题"></a>CUBLAS加速矩阵乘法的一个小问题</h2><p>矩阵、张量在内存和显存中均以一维方式存储。</p>
<p>在CUBLAS中，GPU的计算单元读取矩阵的方式是按列读取和放回显存。我们习惯内存、显存以及cpu以行优先的方式读取。CUBLAS提供的API就不会很习惯，因此做一个封装使得外界使用时仍然按照习惯写法，但是内部仍用CUBLAS加速。</p>
<p>采用的方式调用CUBLAS的API的时候将A和B顺序反过来。这是因为CUBLAS读取的是转置，若$C=AB$，则$C^T=B^T \times A^T$，而$C^T$在GPU计算单元写回的时候是列优先顺序，则按照行优先读取的时候就是$C$，所以在封装内部调用CUBLAS的API的时候将A和B顺序反过来即可。</p>
<p>代码如下</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">acc_gemm</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">const</span> cublasOperation_t cuTransA, <span class="keyword">const</span> cublasOperation_t cuTransB, </span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">const</span> <span class="keyword">int</span> M, <span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">int</span> K, </span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">const</span> <span class="keyword">float</span> alpha, <span class="keyword">const</span> <span class="keyword">float</span>* A, <span class="keyword">const</span> <span class="keyword">float</span>* B, </span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">const</span> <span class="keyword">float</span> beta, <span class="keyword">float</span>* C</span></span></span><br><span class="line"><span class="function"><span class="params">)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> ldb = (cuTransB == CUBLAS_OP_N) ? N : K;</span><br><span class="line">  <span class="keyword">int</span> lda = (cuTransA == CUBLAS_OP_N) ? K : M;</span><br><span class="line">  cublasHandle_t cublas_handle_;</span><br><span class="line">  CHECK_CUBLAS(cublasCreate(&amp;cublas_handle_));</span><br><span class="line">  CHECK_CUBLAS(cublasSgemm(cublas_handle_, cuTransB, cuTransA, N, M, K, &amp;alpha, B, ldb, A, lda, &amp;beta, C, N));</span><br><span class="line">  CHECK_CUBLAS(cublasDestroy(cublas_handle_));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="caffe中blob的初始化方式"><a href="#caffe中blob的初始化方式" class="headerlink" title="caffe中blob的初始化方式"></a>caffe中blob的初始化方式</h2><p>caffe，又称blobflow。</p>
<p>caffe是用来实现各种各样神经网络前向推理，和反向传播的框架。神经网络由各种layer组合构成，caffe自身提供了卷积、全连接等层的实现。</p>
<p>由于layer的是存储密集、计算密集型的任务，且适用并行加速，因此涉及到cpu、内存、显存和gpu之间的切换。另外layer的输入、输出，自身权重都是高维张量构成，而数据在内存和显存的存储为一维结构，因此维度、数据的长度、显存和内存的同步这三个因素需要被考虑。</p>
<p>caffe用blob表征权重、输入和输出等张量。blob逻辑上描述了张量维度、展平后的数据长度。也描述了blob数据对应的导数数据，如果只做前向传播的话这部分数据是用不到的。blob的默认初始化不分配内存和显存，张量维度和展平后数据长度默认为0。blob的显示初始化对应分别对应权重张量和输入输出张量的存储分配。</p>
<p>权重的张量维度、展平后的数据长度在模型文件中被定义，因此caffe定义第一种初始化和保存方式：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">FromProto</span><span class="params">(<span class="keyword">const</span> BlobProto&amp; proto, <span class="keyword">bool</span> reshape = <span class="literal">true</span>)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ToProto</span><span class="params">(BlobProto* proto, <span class="keyword">bool</span> write_diff = <span class="literal">false</span>)</span> <span class="keyword">const</span></span>;</span><br></pre></td></tr></table></figure>
<p>针对前向推理以及反向传播的中间计算结果、即输入张量和输出张量，caffe定义了第二种初始化方式：这种初始化方式调用了Reshape函数，Reshape当中会为数据分配内存和显存。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">explicit</span> <span class="title">Blob</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; shape)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line">Blob&lt;Dtype&gt;::Blob(<span class="keyword">const</span> <span class="keyword">int</span> num, <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> width)</span><br><span class="line">  <span class="comment">// capacity_ must be initialized before calling Reshape</span></span><br><span class="line">  : capacity_(<span class="number">0</span>) &#123;</span><br><span class="line">  Reshape(num, channels, height, width);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>和第一种方式的区别是这种方式只定义张量维度，不对数据的值做要求，采用这种接口的原因是中间张量的数据并不是我们需要保存的权重，只是需要在显存中开辟张量存储的空间。</p>
<p>由于前向推理时大部分Layer的设计中，（1）输出张量的维度依赖输入张量的维度，（2）并且上一层的输出张量一般是下一层的输出张量，因此Layer在初始化时，一般要求输入张量bottom的维度确定并完成内存显存分配，在调用SetUp函数时再确定输出张量top的维度并完成存储空间分配。多个Layer构成的推理流在初始化时，也要遵循这一设定。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">SetUp</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;Blob*&gt;&amp; bottom, <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;Blob*&gt;&amp; top)</span> </span>&#123;</span><br><span class="line">  CheckBlobCounts(bottom, top);</span><br><span class="line"><span class="comment">// bottom blobs need to be allocated shape and data memory before, where top blobs don&#x27;t.</span></span><br><span class="line">  LayerSetUp(bottom, top); </span><br><span class="line">  Reshape(bottom, top);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="CMake编译系统"><a href="#CMake编译系统" class="headerlink" title="CMake编译系统"></a>CMake编译系统</h2><p>环境gcc7.5.0，ubuntu18.04，cmake3.25.0。</p>
<p><img src="/img/Cmake编译系统/1.png" alt=""></p>
<p>整体结构如上图所示，u1文件夹里是写好的算法代码，eval文件夹里是基于算法编写的应用程序，包括测试、实验对比、调用程序等。<br>因为用到protobuf，所以需要先对proto文件处理生成对应*.pb.cc和*.pb.h文件，这两个文件内容比较多，先把其编译成独立的库，而不是直接和u1里的代码混编，第二步编译u1，最后编译eval里的应用程序。</p>
<p>有几个点注意：第一头文件和执行文件放在一起，所以不设置单独include文件夹，第二cmake版本小于3.10需要手动引入cuda库。</p>
<p>因此CMakeLists.txt的书写方法如下：</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.10</span>)</span><br><span class="line"><span class="keyword">project</span>(u1_test CXX C CUDA)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>先声明项目需要用到语言，因为是GCC7.5.0，默认C++11标准。</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">include_directories</span>(<span class="string">&quot;$&#123;PROJECT_SOURCE_DIR&#125;/src&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">aux_source_directory</span>(<span class="string">&quot;$&#123;PROJECT_SOURCE_DIR&#125;/src/u1&quot;</span> u1)</span><br></pre></td></tr></table></figure>
<p>声明头文件额外引入路径和需要编译的库目录</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">find_package</span>(gflags REQUIRED)</span><br><span class="line"><span class="keyword">find_package</span>(glog REQUIRED)</span><br><span class="line"><span class="keyword">find_package</span>(protobuf REQUIRED)</span><br></pre></td></tr></table></figure>
<p>声明需要用到的包，glog，gflags和protobuf是我做C++开发比较喜欢用的库。</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># protoc -I=. --cpp_out=. kernel.proto</span></span><br><span class="line"><span class="keyword">set</span>(proto_file_name <span class="string">&quot;kernel&quot;</span>)</span><br><span class="line"><span class="keyword">get_filename_component</span>(u1_proto <span class="string">&quot;$&#123;PROJECT_SOURCE_DIR&#125;/src/u1/proto/$&#123;proto_file_name&#125;.proto&quot;</span> ABSOLUTE)</span><br><span class="line"><span class="keyword">get_filename_component</span>(u1_proto_path <span class="string">&quot;$&#123;u1_proto&#125;&quot;</span> PATH)</span><br><span class="line"><span class="keyword">set</span>(PROTO_GENERATE_DIR <span class="string">&quot;$&#123;PROJECT_SOURCE_DIR&#125;/src/u1/proto&quot;</span> )</span><br><span class="line"><span class="keyword">set</span>(u1_proto_srcs <span class="string">&quot;$&#123;PROTO_GENERATE_DIR&#125;/$&#123;proto_file_name&#125;.pb.cc&quot;</span>)</span><br><span class="line"><span class="keyword">set</span>(u1_proto_hdrs <span class="string">&quot;$&#123;PROTO_GENERATE_DIR&#125;/$&#123;proto_file_name&#125;.pb.h&quot;</span>)</span><br><span class="line"><span class="keyword">add_custom_command</span>(</span><br><span class="line">  OUTPUT <span class="string">&quot;$&#123;u1_proto_srcs&#125;&quot;</span> <span class="string">&quot;$&#123;u1_proto_hdrs&#125;&quot;</span></span><br><span class="line">  <span class="keyword">COMMAND</span> protoc</span><br><span class="line">  ARGS </span><br><span class="line">    --cpp_out <span class="string">&quot;$&#123;PROTO_GENERATE_DIR&#125;&quot;</span> </span><br><span class="line">    -I <span class="string">&quot;$&#123;u1_proto_path&#125;&quot;</span></span><br><span class="line">    <span class="string">&quot;$&#123;u1_proto&#125;&quot;</span></span><br><span class="line">  DEPENDS <span class="string">&quot;$&#123;u1_proto&#125;&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>生成kernel.proto对应的c++ API文件</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">add_library</span>(</span><br><span class="line">  proto_ttt</span><br><span class="line">  <span class="variable">$&#123;u1_proto_srcs&#125;</span></span><br><span class="line">  <span class="variable">$&#123;u1_proto_hdrs&#125;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">target_link_libraries</span>(</span><br><span class="line">  proto_ttt</span><br><span class="line">  protobuf::libprotobuf</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_library</span>(</span><br><span class="line">  u1_lib</span><br><span class="line">  <span class="variable">$&#123;u1&#125;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>然后分别生成库文件，包括定义接口的库以及算法库。</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span>(add_executable_app app_name app_path)</span><br><span class="line">  <span class="keyword">add_executable</span>(</span><br><span class="line">    <span class="variable">$&#123;app_name&#125;</span></span><br><span class="line">    <span class="variable">$&#123;app_path&#125;</span></span><br><span class="line">  )</span><br><span class="line">  <span class="keyword">target_link_libraries</span>(</span><br><span class="line">    <span class="variable">$&#123;app_name&#125;</span></span><br><span class="line">    u1_lib</span><br><span class="line">    gflags</span><br><span class="line">    glog</span><br><span class="line">    protobuf::libprotobuf</span><br><span class="line">  )</span><br><span class="line"><span class="keyword">endfunction</span> ()</span><br><span class="line"></span><br><span class="line">add_executable_app(<span class="keyword">test</span> <span class="string">&quot;$&#123;PROJECT_SOURCE_DIR&#125;/src/eval/test.cu&quot;</span>)</span><br><span class="line">add_executable_app(main <span class="string">&quot;$&#123;PROJECT_SOURCE_DIR&#125;/src/eval/main.cu&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>最后生成应用程序、测试程序和用来比较的实验数据。</p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>现实世界中的<strong>诸多任务可以被抽象</strong>成一个接收一定形式的输入，并产生某种类型输出的<strong>函数</strong>。然而对于现实世界中的大部分任务，其函数关系复杂且普遍具有<strong>非线性关系</strong>，无法轻松找到这样的函数。</p>
<p>神经网络算法的目的就是通过神经网络结构表征任务的非线性特征，并通过已有数据集产生神经网络参数，从而<strong>拟合</strong>现实世界任务对应的函数，这个函数可以被表示为</p>
<p><img src="/img/Gradient-Descent/func.png" alt=""></p>
<p>其中$structure$表示神经网络的结构，该结构是神经网络拟合现实任务非线性特征的关键，通常情况下是<strong>先验</strong>的，需要研究人员精心设计得到，比如transformer和conv结构。</p>
<p>$X,Y$表示数据集中的输入和输出，参数$w$是神经网络参数。在<strong>模型获取</strong>阶段（训练），我们能够同时拿到数据集的输入和输出$X,Y$，并希望得到神经网络参数$w$；在<strong>模型应用</strong>阶段（推理），我们只拿到数据集中的$X$（输入数据），希望得到和真实世界任务尽可能相近的结果$Y$。</p>
<p>在模型推理阶段中，由于我们已经得到了神经网络参数$w$，获得结果的过程是一个<strong>前向计算</strong>的过程，<strong>按步骤</strong>执行即可。</p>
<p>而在模型训练阶段，神经网络参数$w$是未知的，而函数$O(X)$的输出是已知的，由于神经网络参数$w$通常是高维向量，数据集$X,Y$同样具有一定规模，这相当于是在<strong>近似求解一个高维方程</strong>，这个问题通常是难解的，因此需要梯度下降，也就是<strong>反向传播</strong>过程。</p>
<p>将近似求解高维方程的问题转化为最小化loss函数的问题，令</p>
<p><img src="/img/Gradient-Descent/loss.png" alt=""></p>
<p>在反向传播过程当中，由于数据集中的输入和输出都是已知的，即让该函数最小即可，根据优化理论，按照下式不断迭代，直到loss函数的变化小于某阈值即可。</p>
<p><img src="/img/Gradient-Descent/gd.png" alt=""></p>
<p>这里讨论一些梯度下降方法中确保准确率的一些难点。</p>
<ul>
<li><p>关于数据集：由于数据集是通过实践获得的，只有<strong>统计意义</strong>上的理论保证。所以在每一轮迭代过程中如何选取数据集，如何保证数据集之间的偏差在一定范围内？</p>
</li>
<li><p>关于神经网络结构：由于神经网络的层状结构，存在梯度消失，梯度精度等问题</p>
</li>
<li><h2 id="Lipschitz-Continuity"><a href="#Lipschitz-Continuity" class="headerlink" title="Lipschitz Continuity"></a>Lipschitz Continuity</h2></li>
</ul>
<p>In mathematical analysis, Lipschitz continuity, named after German mathematician Rudolf Lipschitz, is a strong form of uniform continuity for functions. Intuitively, a Lipschitz continuous function is limited in how fast it can change: there exists a real number such that, for every pair of points on the graph of this function, the absolute value of the slope of the line connecting them is not greater than this real number; the smallest such bound is called the Lipschitz constant of the function (or modulus of uniform continuity). For instance, every function that has bounded first derivatives is Lipschitz continuous.</p>
<blockquote>
<p>We have following chain of strict inclusions for functions<br>over <strong>a closed and bounded non-trivial interval</strong> of the <strong>real line</strong> (So strong!)</p>
<p>Continuously Differentiable $\subset$ Lipschitz Continuous<br>$\subset$ $\alpha-$ Holder Continuous $\subset$ Uniformly Continuous = Continuous</p>
</blockquote>
<p>Lipschitz Continuity Defintion:</p>
<p>Given two metric spaces $(X, dX)$ and $(Y, dY)$, where $dX$ denotes the metric on the set $X$ and $dY$ is the metric on set $Y$, a function $f : X \rightarrow Y$ is called Lipschitz continuous if there exists a real constant $K \geqslant 0$ such that, for all $x_1$ and $x_2$ in $X$,</p>
<script type="math/tex; mode=display">dY(f(x_1), f(x_2)) \leqslant KdX(x_1, x_2)</script><h2 id="Metric-Space"><a href="#Metric-Space" class="headerlink" title="Metric Space"></a>Metric Space</h2><p>In mathematics, a metric space is a nonempty set together with a metric on the set. The metric is a function that defines a concept of <em>distance</em> between any two members of the set, which are usually called points. </p>
<p>The metric satisfies a few simple properties:</p>
<ul>
<li><p>the distance from $A$ to $B$ is zero if and only if A and B are the same point.</p>
</li>
<li><p>the distance between two points are positive.</p>
</li>
<li><p>the distance from $A$ to $B$ is the same as the distance from $B$ to $A$, and</p>
</li>
<li><p>the distance from $A$ to $B$ is less than or equal to the distance from A to B via any third point C.</p>
</li>
</ul>
<h2 id="Covering-Number-and-Packing-Number"><a href="#Covering-Number-and-Packing-Number" class="headerlink" title="Covering Number and Packing Number"></a>Covering Number and Packing Number</h2><p>In mathematics, a covering number is the number of spherical balls of a given size needed to completely cover a given space, with possible overlaps. </p>
<p>Two related concepts are the packing number, the number of disjoint balls that fit in a space, and the metric entropy, the number of points that fit in a space when constrained to lie at some fixed minimum distance apart.</p>
<p>Let $(Z, d)$ be a metric space, let $K$ be a subset of $Z$, and let r be a positive real number. Let $B_r(x)$ denote the ball of radius $r$ centered at $x$. A subset $C$ of $Z$ is an $r-external$ covering of $K$ if:</p>
<script type="math/tex; mode=display">K \subseteq \cup _{x\in C} B_r(x)</script><p>In other words, for every $y \in K$ there exists $x \in C$ such that $d(x,y) \leqslant r$.</p>
<p>If furthermore $C$ is a subset of $K$, then it is <em>an r-internal covering</em>.</p>
<p>The <em>external covering number</em> of $K$, denoted $N_{r}^{\text{ext}}(K)$, is the minimum cardinality of any external covering of K. The <em>internal covering number</em>, denoted $N_{r}^{\text{int}}(K)$, is the minimum cardinality of <strong>any</strong> internal covering.</p>
<p>A subset $P$ of $K$ is a packing if $P \subseteq K$ and the set ${B_{r}(x)}_{x\in P}$ is pairwise disjoint. The packing number of $K$, denoted $N_{r}^{\text{pack}}(K)$, is the maximum cardinality of <strong>any</strong> packing of $K$.</p>
<p>A subset $S$ of $K$ is r-separated if each pair of points $x$ and $y$ in $S$ satisfies $d(x, y) \geqslant r$. The metric entropy of $K$, denoted $N_{r}^{\text{met}}(K)$, is the maximum cardinality of <strong>any</strong> r-separated subset of $K$.</p>
<p>Covering number and packing number will be written as $N(Z, d, r), M(Z, d, r), $ respectively.</p>
<p>Metric entropy also equals to $log N(Z, d, r)$.</p>
<p><img src="/img/Metric-Space/coveringAndPacking.png" alt=""></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>归档</p><p><a href="https://github.com/anonymity1/anonymity1.github.io/2022/02/03/归档/">https://github.com/anonymity1/anonymity1.github.io/2022/02/03/归档/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Hesheng Sun</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2022-02-03</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2024-08-07</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/%E7%9E%8E%E6%89%AF%E6%B7%A1/">瞎扯淡</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2022/12/29/%E6%96%B0%E7%AF%87%E7%AB%A0/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">新篇章</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2021/02/14/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%A0%91%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95/"><span class="level-item">蒙特卡洛-树搜索方法</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/my_avatar.png" alt="anonymity1"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">anonymity1</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">15</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">0</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">4</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/anonymity1" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/anonymity1"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com/hesheng.sun"><i class="fab fa-facebook"></i></a></div></div></div><!--!--><!--!--><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-07-29T06:04:01.000Z">2024-07-29</time></p><p class="title"><a href="/2024/07/29/%E8%AE%A4%E7%9F%A5%E3%80%81%E7%BB%84%E7%BB%87%E3%80%81%E5%86%B3%E7%AD%96%E5%92%8C%E6%89%A7%E8%A1%8C/">认知、组织、决策和执行</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-06-07T07:28:55.000Z">2024-06-07</time></p><p class="title"><a href="/2024/06/07/2023%E5%B9%B4-2024%E5%B9%B4%E5%85%AD%E6%9C%88%E6%80%BB%E7%BB%93/">总结：2023-2024.06</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-01T10:07:43.000Z">2024-04-01</time></p><p class="title"><a href="/2024/04/01/%E5%AE%9E%E8%B7%B5%E5%92%8C%E9%9B%86%E4%BD%93%E5%B1%9E%E6%80%A7/">实践和集体属性</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-01-22T08:58:08.000Z">2024-01-22</time></p><p class="title"><a href="/2024/01/22/%E8%AE%A4%E7%9F%A5%E7%9A%84%E5%AE%9E%E8%B7%B5/">认知的实践</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-12-20T14:01:15.000Z">2023-12-20</time></p><p class="title"><a href="/2023/12/20/%E9%9B%86%E4%BD%93%E5%B1%9E%E6%80%A7/">认知的传播来源和集体属性</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/07/"><span class="level-start"><span class="level-item">July 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/06/"><span class="level-start"><span class="level-item">June 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/04/"><span class="level-start"><span class="level-item">April 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/01/"><span class="level-start"><span class="level-item">January 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/12/"><span class="level-start"><span class="level-item">December 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/11/"><span class="level-start"><span class="level-item">November 2023</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/08/"><span class="level-start"><span class="level-item">August 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">December 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/02/"><span class="level-start"><span class="level-item">February 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/02/"><span class="level-start"><span class="level-item">February 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/01/"><span class="level-start"><span class="level-item">January 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/%E6%80%BB%E7%BB%93/"><span class="tag">总结</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%9E%8E%E6%89%AF%E6%B7%A1/"><span class="tag">瞎扯淡</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AC%94%E8%AE%B0/"><span class="tag">笔记</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%9A%8F%E7%AC%94/"><span class="tag">随笔</span><span class="tag">5</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">anonymity1</a><p class="is-size-7"><span>&copy; 2024 Hesheng Sun</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Github" href="https://github.com/anonymity1/anonymity1.github.io"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>