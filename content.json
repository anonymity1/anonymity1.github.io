{"pages":[],"posts":[{"title":"8月摸鱼","text":"八月划水 [TOC] 短跑100米苏炳添半决赛9.83秒破亚洲纪录，32岁老将，放在里约奥运会和博尔特同台竞技也是银牌水准，在我眼里其历史地位接近刘翔。于是好奇查了查这个人的相关资料，竟发现他还发表了不少论文和励志小故事，读了一些他发表的文章，感觉有趣，遂记录。 一篇论文苏炳添在2019年发表的一篇关于100米短跑论文，本篇论文非常有趣，一些观点也很有启发意义。 新时代中国男子 100m 短跑：回顾与展望 123456789@article{苏炳添2019新时代中国男子, title={新时代中国男子 100m 短跑: 回顾与展望}, author={苏炳添 and 邓民威 and 徐泽 and 梁伟 and 姜自立 and 王国杰}, journal={体育科学}, volume={39}, number={2}, pages={22--28}, year={2019}} 新时代这个词可以说很中国特色了，全文累计出现32次，在第五部分中国男子100m短跑短跑发展对中国竞技体育的启示中出现的尤其多。 以笔者为例是不是体育类论文就可以这么干？前段时间帮老板找了找关于掷铁饼的论文，里面也是以个别优秀运动员为例进行分析 科学训练的作用 所谓科学化训练理念，是指符合专项竞技能力发展规律、人体生理机能状态变化规律和运动员个人特点的训练理念。美国是世界公认的科学化训练程度最高的国家，笔者的现任教练 Randy Huntington，谢震业和韦永丽的现任教练 Rana Reider 均 是 来 自 美 国 的 著 名 教 练 员 。 Randy Huntington 和 Rana Reider 都是典型的“科研型教练”，他俩的整体训练思路是：以“冠军模型”为指导，通过高科技仪器和设备对运动员体能、技术、恢复等各个环节进行全方位监控，据此发现问题，寻找差距，制订个性化的训练方案，进而恶补短板，全面提升运动员的竞技能力。 看看用到的设备和监测指标 一般人也玩不起哈哈哈，另外从训练的作用来看，和高水平研究机构的合作非常重要，在突破自我和人类极限上，甚至起决定性作用。舆论有些民族主义（外宣内宣化）的情况下，还是要认识到别人能牛b的根本因素是什么。 评价总结6点中国团体100米跑成功经验，给出4条未来发展方略，有定量分析，也有一些事例分析，但是事例分析更多是以见闻这种形式阐述，一般论文这么写可能需要加一些数据，但毕竟作者是跑神哈哈哈。 一篇小故事经典鸡汤，没想到苏神不仅能跑步，写论文，还能煨鸡汤。 1234567891011121314151617# 中华儿女@article{苏炳添2018演绎, title={演绎 “中国速度”}, author={苏炳添}, journal={中华儿女}, number={4}, pages={4--4}, year={2018}}# 意林@article{苏炳添2019我必须追上去, title={我必须追上去}, author={苏炳添}, journal={意林 (原创版)}, year={2019}} 两篇文章是一样的可能意林抄中华儿女的，还是苏神一稿多投（bushi）？毕竟中华儿女是共青团中央主管，总设计师还提过词，而意林是长春市委宣传部主管。 跑神竟也是鸡汤大师 现在，我的竞争对象变成了自己。我的下一个目标，就是要突破9.90秒的成绩。这是一个大关，就像破 10 秒一样。我希望自己能够成为国内第一个突破 9.90秒的运动员。短跑一直被认为是挑战速度的极限。对于我来说，人不应该给自己设限，而要看看能不能逼自己做出新的成绩。突破自我极限的方法很简单，就是好好训练。 当你在特别累、特别无聊、特别枯燥的时候，想一想自己的目标，你就不累了。我的目标就是跑得更快。 一闻就是鸡汤的味道哈哈哈，还是论文读的舒服，老老实实讲了讲自己怎么样好好训练，怎么样能跑的快。或者是鸡汤文篇幅受限，不能讲具体的东西，只能传递一些看起来正确却又没什么帮助的大道理？","link":"/2021/08/02/8%E6%9C%88%E6%91%B8%E9%B1%BC/"},{"title":"NaiveGo","text":"简单的五子棋对战程序 从幼儿园开始学习围棋，一直到上初中之前，围棋可以说是我童年时代的最大爱好，依靠围棋也拿了不少奖项。可惜初中开始学业繁忙，围棋逐渐荒废，直到2016年上大学才算有些空闲时间，又恰逢AlphaGo横空出世，击败李世石和柯洁，打破了围棋是无法被AI求解的这一说法，算是重新捡起了围棋。因为自身对围棋的喜欢加上对所谓AI如何击败人类的好奇，从大学选择计算机专业开始，一直以来都想实现一个这样一个能够打败人类的围棋AI。限于本科阶段积累的知识和能力还不够，这个目标直到2021年寒假才算实现了非常小的一部分，也就是这个简单的五子棋程序 这是一个目标不断减小的过程，最开始打算实现一个简单的围棋AI，发现训练19路棋盘需要的算力太过庞大，然后考虑9路棋盘，这仍然足够庞大。加上围棋规则比较复杂，于是将目标转向五子棋，先打算整一个19路棋盘的五子棋，尽管五子棋规则简单，但训练数据量仍然过于庞大，于是缩小棋盘，弄了一个8x8棋盘的五子棋，然后迁移到10x10棋盘上。为什么是8x8呢，因为有位前辈开源了他对这样一个五子棋AI的实现，在8x8的五子棋棋盘有比较好的实现效果。有前人指路，自己写这样一个AI还是有一些信心的。至于更加艰难，在简单笔记本上难以实现的围棋AI，那就等以后有空再实现吧。 经验 Google+英文搜索通常能够找到适用于当前的解答。 在进行检索的过程当中，官方文档可以解决一些API如何调用，接口设定的问题，各种overflow可以解决一些非常神奇的bug，如果需要解决一些项目特有的问题，去看github的issue比较好。至于博客，尽管我也写博客，但不得不说，大部分问题的解决不用去翻博客，博客更多的不是用来解决问题，而是分享个人感受。最后吐槽一下用中文搜索获得的搜索内容，经常是翻到一些参差不齐的博客，然后发现这些博客内容差不多，对于解决问题来说效率很低。 实现算法密度特别高的项目需要2-5个小时的集中精力才能使项目有进展，需要提升专注力。 这也可能是我个人原因吧，坐下之后往往需要很长时间才能续接之前的思路，之后如果投入时间不长的话进度就会很微弱。之后要有意识提升自己的专注能力，感觉小学时候下围棋和初中做几何题时候非常专注，是不是当有一些明确目标和思路的时候自己的专注程度会提升呢？ 大致设计一个规划可以使得进度有保障，但是要对进度推迟有心理预期。 这应该就是之前讨论的专注程度还不大够导致的吧，需要有一个规划确保一下思路是明确的。自己大部分时候设定的计划预期偏高，所以进度推迟可能比较常见，不需要有太大心理落差，这样当目标完成之后反而会非常快乐。 确保完成每个模块后进行一个可视化效果较好的测试。 程序输出helloworld并不困难，但是要求算法既鲁棒性好又能力强就超级困难。如果希望在最终任务实现上效果比较好，那就尽量每个模块完成之后都测试比较多的数据，这样当执行最终任务时，可以比较快速确定bug的位置，从而加快项目进度。","link":"/2021/03/16/NaiveGo/"},{"title":"Hello World","text":"2021年1月26日 星期二 看到徐哥博客写的不亦乐乎，在复习计算复杂性时受益良多。本菜鸡表示对此心生神往，于是在2021年寒假空闲时简单搭个博客，记录一些闲暇时的感悟。 附言顺便说一下，徐哥写了不少计算复杂性的博客，还向我秀了一波成绩，但还是没我高:-)，（当然我没告诉他最后我的分数）。","link":"/2021/01/26/hello-world/"},{"title":"HiveD","text":"HiveD: Sharing a GPU Cluster for Deep Learning with Guarantees (OSDI’ 2020)，北大，Microsoft，港大合作的一篇文章。 多租户在GPU集群上进行DL训练是一种较为常见的场景，然而目前GPU集群资源分配是基于quota的，即每个任务指定需要的GPU数目，这样的粗粒度资源调度方式容易造成更长的累积延迟，本文因此提出了一种更细粒度GPU集群资源调度方式。 动机这是因为现有在公共GPU集群上为多租户分配训练资源的方式是基于quota的，即分配GPU的数目。公共GPU集群调度器为租户和私有集群里一样多的GPU数目，但是这样的GPU效率可能由于某些原因（单一node计算能力不强，没有足够GPU）造成当前租户任务完成时间增加，从而产生队列累积延迟。 公共GPU集群上可能会有多个node，每个node上有多个GPU。如果一个DL任务需要64个GPU，那么集群调度器产生一个8node，每个node分配8GPU的对应affinity。同时会有些很小的DL任务，这些任务可能只需要1-3个GPU，但是这样的DL任务也会占据一个node，由于很多的node被这样的小DL任务占据，集群调度器无法为需要64个GPU的DL任务的affinity找到对应的配置，所以这个大DL任务只能等待，由此产生了累积延迟。 这个问题似乎是可以被解决的，有些类似于内存管理中的memory fragmentation问题。 （这个动机似乎有点easy），不过底层资源没有池化，而是分层存在，确实是一个现实的问题。 解决方案本文提出了HiveD系统，确保共享GPU执行时是safety的。HiveD将GPU资源分为Virtual Private Cluster（VC）和physical Cluster两层。HiveD将每个租户表示为一个VC，每个VC会预先安排一系列的cell，这里的cell表示需要多少资源。 cell里的资源是分层的，这篇文章作者将资源分为5层：GPU -&gt; PCIe switch -&gt; CPU socket -&gt; node -&gt; Rack，通过分层实现细粒度资源调度。（这个分层有些疑惑，同时每一层内各个cell之间的通信恐怕也需要考虑） 如果将资源认为是池化的，一些常规的调度器，比如k8s default schedular会根据任务需要资源的数量直接分配资源，但是因为GPU集群里资源没有池化，所以需要再封装一层，将node以及更细粒度的资源池化，这也是这篇文章的核心idea。 同时由于分层资源的塔式结构，底层资源与上层资源有隶属关系，需要根据这种隶属关系调整分配的细粒度资源。这篇文章称之为buddy cell，当所有的buddy cell都是空闲的时候，那么就可以合并成一个更粗粒度的资源，在分配细粒度的资源时，通过考虑这种隶属关系，尽可能保持粗粒度的资源没有被占用，从而保证大的DL任务可以被正常执行而不用延迟。 最后是关于低优先级VC，这类VC可以被随时抢占，所以分配cell时，优先分配到那些没有被大DL任务占据的buddy cell当中，这样可以保证大概率不被抢占，同样对于guaranteed VC，优先分配buddy cell里没有低优先级任务的VC，从而降低抢占发生的次数。 实验环境2019年11月实际部署在openPAI项目中，集群当中有800多台GPU，Nvidia和AMD的，也包括200多台Azure GPU虚拟机。执行的任务包括NLP任务（BERT），AutoML实验，以及几百个单GPU任务。 项目构成：7700行golang代码，以及js，shell脚本和yaml配置文件。 工作流：和k8s的默认调度器配合工作，能够复用k8s调度器的基本逻辑和相关配置。 trace-based experiment: USENIX Annual Technical Conference (ATC’ 19） GPU cluster: 96-GPU cluster 部署在Azure上（24台NC2，NC2：4 个NVIDIA K80） 对比算法：没有用HiveD，而使用Quota的YARN-CS，Gandiva，Tiresias 启发应用到NUMA架构或者非PS架构的GPU集群，云当中应用HiveD。","link":"/2021/04/06/osdi2020-HiveD/"},{"title":"clockwork","text":"Serving DNNs like Clockwork: Performance Predictability from the bottom up (OSDI’ 2020) ，德国Max-Planck 软件系统研究所和美国Emory university合作的一篇文章。 由于卷积运算的复杂性，在应用层面执行DNN模型推理可能会存在一定的时延，当同时有多个DNN模型需要执行的时候，调度不同的DNN模型就会产生累计时延和尾延迟（tail latency），从而导致无法满足SLO（service-level objectives）。本文观察到DNN模型的执行时间是可以预测的，但是由于现有系统在多个层次上执行不同的调度策略，导致预估DNN模型执行时间非常困难，因此本文从底层构建了一个DNN推理模型调度系统，实现对DNN模型的执行时间预测，从而实现了更高质量的DNN模型调度。 动机和方法预测DNN推理模型执行时间对于执行。为什么已有系统难以实现对DNN推理模型执行时间的预测？从底层的Hardware level，OS level到顶层的Application level，在硬件层面指令乱序执行，操作系统层面各个进程竞争执行，以及应用层面竞争需要的计算存储资源，多级的竞争和调度使得预测一个DNN推理模型的执行时间非常艰难，所以这篇文章提出打破各个层面的松耦合，构建一个紧耦合系统，实现在顶层的DNN调度，从而可以确保对推理模型执行时间的预测。 实验宣称26 KLOC C++代码，实验环境私人小型集群。 12 servers：每个server有32 cores，768G RAM, 2 v100 GPUs 感受各种奇葩单词看的我有点难受，阅读体验极差，没有找到开源实现，影响力有待验证。","link":"/2021/04/07/osdi2020-clockwork/"},{"title":"q-learning","text":"强化学习并不新鲜。 2017年AlphaZero出来之后，一堆自媒体狂吹强化学习有多么牛b，利用DQN吊打普通的AlphaGo（说来惭愧，那阵自己还关注新智元和机器之心这种公众号）。 尽管好奇，可由于课程等诸多原因，一直没有闲暇的时间关注RL，直到2021年需要在研究领域用到强化学习算法，就简单了解和实践了一下，发现这个算法的核心思想其实也蛮简单，没有各路自媒体吹的那么神奇，于是将自己的一些理解简单记录一下。 智能体和环境人们总是希望创造出一种通用智能体，能够通过和环境不断的交互，在实现目标的过程当中，自己总结经验，得出与环境交互最优的方法。 这种智能体与人类的思维模式很类似，因为从婴儿牙牙学语到高中生根据高考大纲有意识培养自己的应试能力，人类一直在做的事情就是适应环境，找到在当前环境下解决问题的最优方式。 但是环境是复杂多变的，经验是抽象的，与环境的交互是有代价的。所以对于计算机来说，如何用比特流表征经验，环境和代价是一个困难的问题。 其实到这里，强化学习的概念就算是介绍完了，强化学习就是智能体实现和环境智能交互的方法。剩下的就是经验，环境以及最优如何被形式化地定义的问题。 总结一下， 1. 强化学习的最终目标是使得智能体面对复杂环境能够实现某种决策方案，使得某个目标最优。 2. 强化学习算法已知的信息只有智能体和环境交互的规则和代价。 3. 强化学习的难点在于如何表征经验，环境，定义最终目标以及智能体更新“经验”的算法。 数学表示环境和智能体整体状态的集合表示：$s \\in S$，每个状态$s$都包含环境和智能体两者的状态信息。 智能体和环境交互动作的集合表示：$a \\in A$ 智能体采取某种动作后整体状态发生变化，变化可能不确定，用概率分布表示：$Pa(s, s^{‘}) = Pr(s{t+1} = s^{‘} | s_t = s, a_t = a)$，表示时刻t智能体执行动作a后整体状态从s到$s^{‘}$的概率。 智能体和环境交互后会有一个反馈：$ra(s, s^{‘})$，表示执行动作a，整体状态从s到$s^{‘}$后，智能体得到的反馈R，我们假设这个R为标量，值越大动作策略越好。由于在状态s下执行动作a迁移到的状态不确定，所以$R_a(s) = \\sum{s^{‘}\\in S} P_a(s, s^{‘})r_a(s, s^{‘})$ 优化目标的定义：首先我们定义智能体和环境的交互策略$\\pi$，事实上$\\pi$就是我们要优化的变量，即不同环境状态下该如何选择动作。在交互策略$\\pi$下，当前状态的累计收益为 V^{\\pi}(s) = E[R] = E[\\sum_{t=t_0}^{\\inf}\\gamma^{t}r_t|s_0=s]其中$\\gamma$是衰减因子，最终的优化表达式为$V^{*}(s) = \\max_{\\pi}V^{\\pi}(s)$ q-learning由于探索环境的过程复杂，精确求解$V^{*}(s)$似乎比较困难，有人提出了q-learning算法。 所谓q-learning就是将$\\pi$用一张具有动作和状态两个维度的二维表表示，称为q。智能体处于一个状态时，衡量该状态采取不同动作得到的q值大小，以高概率选择q值大的动作和环境交互。注意q和r的区别：r只代表了当前收益，而q包含了未来的收益。每次执行过后，用如下公式更新q表， Q^{new}(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha (r_a(t) + \\gamma \\max_{a}Q(s_{t+1}, a) - Q(s_t, a_t))其中$\\alpha$表示学习率，值越大更新q表速度越快，q表就是智能体和环境交互的策略。这个公式不难理解，将当前的收益和未来的收益全部统计进入q表，因为未来的收益需要递归展开，而未来的收益难以确定，这里用当前q表里的$\\gamma \\max{a}Q(s{t+1}, a)$来表征未来收益，从而简化递归过程。 简单例子环境是一条长度20的赛道，终点在赛道右侧。智能体可以选择的动作为向左或向右，初始阶段智能体位于赛道最左侧，目标是达到赛道终点。 显然达到这个目标的算法非常简单，智能体直接向右走，判定走过的点是不是终点就行，但是现在智能体不会设计这样的算法，只希望尽快达到终点。假设智能体每走一步r=-0.5，达到终点r=1，我们可以令智能体在和环境交互的过程当中获得经验。 1234567891011121314151617181920212223242526def q_learning(): q_table = build_q_table(N_STATE, ACTIONS) step_counter_times = [] for episode in range(MAX_EPISODES): state = 0 is_terminal = False step_counter = 0 update_env(state, episode, step_counter) while not is_terminal: action = choose_action(state, q_table) next_state, reward = get_env_feedback(state, action) next_q = q_table.loc[state, action] if next_state == 'terminal': is_terminal = True q_target = reward else: delta = reward + GAMMA*q_table.iloc[next_state,:].max() - q_table.loc[state, action] # crucial step q_table.loc[state, action] += ALPHA*delta state = next_state is_terminal, steps = update_env(state, episode, step_counter+1) step_counter += 1 if is_terminal: step_counter_times.append(steps) return q_table, step_counter_times 这个q表大小2*20，但是足够了。首先选择动作，获得反馈后更新q表，重复此过程直到达到终点。这为一个episode，然后反复训练这个智能体更新q表，这个q表就反映了智能体获得的“经验”。 这个例子非常简单，也方便实现，但是当我们的状态数量非常多，难以用一张表表示，这该怎么办呢？ DQN待续","link":"/2021/02/24/q-learning/"},{"title":"信仰和希望","text":"观西游降魔篇有感 玄奘的信仰陈玄奘的信仰是真正的信仰。 他的信仰没有任何欲念作为回报，不断地割舍欲念反而让他的信仰更加真实和坚定。 村镇收取鱼精的经历割舍了他一丝虚荣的念头，猪刚烈的故事告诉他众生的魔念从不是水中浮萍，即使爱情对于他来说也只是一种怀念，而不是难以割舍的一部分。 所以当孙悟空打死玄奘所爱之人，问他所信的佛祖在哪之时，即使心有不甘，意气难平，玄奘仍会坚持双手合十，因为那是他的信仰，玄奘的信仰又岂会因苦难或是欲念改变呢？ 真正的信仰是不能祈求任何回报的，有了回报信仰就会变味为一种交易，这信仰简单的说，大概就是一种希望吧。 无法确定最终的结果好坏，只是朝着看到的那个可能走去，这条道路就仿若没有边际的大海，隐隐约约有一个彼岸的轮廓，可并不知道它在哪里，信者只是因为信仰其存在而朝那里走去，而不能稍有变更。 倘使有丝毫预期回报的可能，这信仰难免沦为一种谋略，最终蜕变成不择手段的夺取，甚至是光荣，也可能腐蚀信仰。没有了世人的景仰，信仰还需坚持吗？以苦海里的挣扎作为回报的投资，抑或是以荣耀作为信仰的期许，那都不是佛祖对于玄奘的期盼，更不是玄奘自己的希望。 世人的信仰陈玄奘的信仰是为众生争渡的大爱和宏愿。 我等世人怕是难以理解和拥有其宽阔的胸襟和宏愿，只是期许在某些想要放弃，容易放弃的时刻，对那些属于自己的信仰，能够再多一点点希望罢。","link":"/2021/02/09/%E4%BF%A1%E4%BB%B0%E5%92%8C%E5%B8%8C%E6%9C%9B/"},{"title":"前20周","text":"一个迟到的总结。 1月17号周日，刚好是开学后第二十周的结尾，18号，也就是第21周的开头，回到东北开启度假模式。 来南京之前，只定了两个目标，保持身体健康和发论文毕业。来到南京这140天，身体素质大概是保持住了，可惜在达成毕业条件这个事情上，有太多的诱惑使我不能保持持续的专注： 两门课分散了精力，计算复杂性和并行算法 认识了一群有趣的小伙伴，老带我玩耍 围棋和摄影总难以舍弃 没有在研究的一个方面持续投入精力 关于课程计算复杂性对于第一次上这课的印象，我只记得双手支起脑袋，努力瞪大眼睛，听到什么图灵机的符号转移之后，大概就昏昏欲睡了，然后就是下课喧闹的声音。 第一堂课确实劝退，不过我却知道了一个事实：林老师脾气不错，让我这混学分的看到了希望。同时自己心底也抱有这样的想法：毕竟也是学计算机的，要是连计算机能处理什么问题，不能处理什么问题都不能有个大概的了解，岂不是白学了？ 看得出林老师非常明白自己要讲的东西，可惜这课确实不太简单，菜鸡如我用一堂课的时间理解什么是BPP复杂度类，什么是PH复杂度类，着实不太容易。就算是对于NP问题定义里的字符串x，语言L和certificate，尽管自己能理解这个是什么意思，可对应到具体的数学语言上，我只想说不管怎么理解都不太对。转折大概是13周吧，在问了老师和徐哥很多关于概念，看起来非常幼稚的问题之后，竟发现自己不仅这些概念弄通了，连带着作业题也会做了，最后的考试林老师也非常给力，7道题都不难，顺利通关。 尽管和完成毕业论文要求的这一目标关系不大，这门课听进去还是蛮有意思的，前辈们关于什么能够计算，什么不能够计算，计算的代价等诸多问题理性而曲折的讨论，确实是计算机领域最为引人入胜的话题。 并行计算诶，一门早八的课，每次上课都难受的要死，老师讲课逻辑还挺清楚，可音调之平淡，让我一个早上犯困的人认真听100分钟，着实艰难。除此之外，这课主要关注的是并发算法的可行性，老师的博士毕业论文是关于可线性化点的（Linearizable point），找这个点可是扎扎实实的NP难问题，全靠对并发数据结构的熟悉和骤然一闪的灵感。可怜我最后竟坚持上完这课到考试，不过这本书《the art of multiprocessing programming》写的倒是有趣，其中一段关于乐观锁的比喻至今还记得。 大意是在乐观锁实现里面，每个线程可以比喻成一辆不同寻常的出租车，怎么不寻常呢？一次有位乘客坐上了这个出租，这出租从来不在红灯处停车，开的飞快无比，突然间这出租在一个绿灯处停了，乘客人傻了，问这司机是不是脑子有泡，司机告诉他：“我是怕也有像我一样的司机，一直闯红灯。” 关于这门课考试，随堂考，大概八开纸8~9页，考试那天俺还迟到5分钟，第一面送分题用了大概40分钟。艹（一种植物），想到俺还有一次作业没交（总成绩10分），心里拔凉拔凉的，还好最后算是过了。当时为啥想不开要上这门课呢，这门课大概就是自觉学得还行，可是出题比较难的那种吧。 其他至于本学期其他课程，花费精力不多：矩阵论出题非常给力，半小时交卷走人（后面那哥们一直抖腿，加上想上厕所，要不也不会这么快）；分布式系统助教师兄给分良心，Raft实验还挺有趣；物联网导论我还真没去过；嗷，对了，值得一提的还有窦老师的前沿课，金句频出，什么凌晨四点接到陈老师（可能是窦老师那个组的负责人）的微信，请窦老师点评自己的一段发言稿录音，窦老师表示只有四个字能评价：“天籁之音”，再加上窦老师那副非常真挚的面容，回想起来非常有趣。 关于课程大概就是这样了，大概以后都不会再认真上一门像计算复杂性那样的课了诶。 关于娱乐开学初去了趟围棋社，果然江苏的下棋氛围还是浓厚，本科时的朋友没有骗我。小小围棋社竟有50人余，围棋也不是只属于棋牌协会一个部门，而是单独社团，想我本科时候那得叫棋牌协会围棋部。社团内部高手也不少，随便拉个人都是5段，不知道是5段贬值了还是我变菜了hhh 浏览器主页上的第一个收藏网站就是弈客围棋，github排在第三个，有时候想去github上找点东西，随手就会打开弈客围棋，以至于观棋或是下棋忘了时间。就算在实验室，感觉后面师兄总在看着我，手也会像具有惯性似的这样做，希望在主要目标完成以前，能够适当控制自己这方面的爱好诶。 至于摄影呢，投入精力不多，但是寻找拍摄地点画的时间也不少，因为摄影也算是去了不少地方，算是比较消耗时间的爱好，之后转过去玩玩人像摄影，权当多接触人吧。 剩下的什么三国杀，狼人杀等各种桌游，还有炉石等游戏，投入的时间也不少，从10周以后，大概每周都会有1到2次。实验室的朋友们非常浪，总带我进行这类活动，下一阶段为了发论文这一最高目标，这类活动怕是要先停一停喽。 关于研究这学期最开始，认识人不多，也没有课，那真是狂读论文的日子，实验室只有我和后面三位师兄常来，最开始读的论文是infocom上的，那个理论和公式是真的多，一整7、8行的优化表达式，有些还不一定对。后来这类论文读了3篇左右，读的俺是头晕眼花，不过也算摸清这类论文的套路了。之后又开始看一些偏系统的论文，sigcomm上的居多，这论文读起来有很多东西需要查资料，可公式确实少，尽管长，可好读一些。后来又去蹭组会听，胜哥的，翘课听了组里博士讨论班，lamda组老师的，周sir的。大部分老师和和气气，不过大多思维非常敏锐，问题很犀利。 印象最深是周sir组会，组会之前有个空桌，上有精致茶具沏好的一壶茶，时间一到，周sir准时赶到，待得学生报告完之后，周sir得着自己学生先挨个提问，最后总结，语言精炼，不算很长，可总结完之后必定是要喝完一杯茶，并再高高地抬起茶壶沏满一杯，非常潇洒。 不久课程开始忙碌起来，老钱给我和zyt弄了个项目，做那个项目去嘞，这个项目可真是个坑，项目方的意思是糊弄个文档，而老钱意思是让我们挖掘问题，可这项目就是K8S套了层壳，找个锤子问题。项目方的场景是将docker容器当虚拟机使，赋予容器更强大的能力，可这就是个伪需求（画外音：那俺直接用虚拟机不行，要啥docker）。大概只有国企的领导脑子进水才会想到这种用法，而只有应付国企的z国特色乙方企业才会接这种需求。只有处于产学研（研究生生产学习）一线的俺们才会研究这个东西，好在老钱还挺够意思，年底发了些补助慰问我们，并且让我们暂停了这个东西。 项目持续大概两个半月，每周都要做一些实验、文档和汇报，分散了我的一部分精力，加上那段时间还各种浪，基本属于研究停滞的状态。这个项目让我看透了一些东西，见微知著，我大概知道z国的一些科创项目（或者说大部分？）具体是怎么回事，其实最开始还是有些失落。当然，路毕竟是人走出来的，希望还是要有的嘛。 吸取这段经历的经验，我决定先搞个跑实验比较简单，容易出活的。想想学期最开始联邦学习看的也不少，这个实验也好做，就先从这个来了，回家乡这几天主要就在干这事，当然，这些就是第20周之后的事了，休假结束再总结吧。 关于一些有趣的人和事毕竟不是日记hhh，就不具体记录了，只能说每个人的想法真是丝毫不同，尊重并尽量理解不同个体的表现和选择吧。 有些时候在一个方向研究久了，比如俺自己，计算机这方面的书看多了，不看其他方向的书，确实容易形成思维定势。想我从初中开始看的书逐渐偏向理工科，本科时看的书更少，这个假期的空闲时间是该狠狠读一波书了。","link":"/2021/01/26/%E5%89%8D20%E5%91%A8/"},{"title":"蒙特卡洛方法","text":"从复杂性角度看，围棋等棋类游戏是典型的NP-hard问题。 严格来讲，棋类问题求解属于PH问题，以围棋为例，解空间约有为$10^{250}$种可能。就算是超级计算机每秒万万亿次计算速度，即每秒能够穷举$10^{16}$种可能，相对于这么大的解空间来说也只是杯水车薪。 所以有人说如果算力足够，这个问题暴力搜索也可以，那ta显然对$10^{234}$这么多倍的差距不够了解。 MCTS原理话说回来，量变引起质变，为了解决这么大的解空间搜索问题，一种随机性的策略被提出来了——Mento Carlo Tree Search。当然，这和蒙特卡洛方法是不一样的，蒙特卡洛树搜索是将蒙特卡洛方法中的随机性引入到搜索树当中。有关文献也将这棵搜索树称为博弈树，因为对于棋类游戏，对弈方的目标是正好相反的。 所以，MCTS算法本质上树的搜索算法，输入是当前局面状态，输出是可能的最优解。 如果你意图使用暴力搜索的方法，只需遍历这棵搜索树，穷举所有可能，思路自然非常简答，但是解空间规模不允许你这样做。MCTS在搜索这棵树的时候引入了随机性。 回想我们下棋的方式，通常是脑海中出现几个可用的选点，对比这些选点的对于局势的影响，选择其中最有利的选点。 对于暴力搜索有两个难点： 1. 如何找到可用选点？ 2. 如何确定这些选点的相对好坏？ 先讨论第二个问题，对于人类来讲，确定选点的好坏往往需要递归的进行这样的思考，直到人类可以“显而易见”看出某个局面的好坏。然而对于机器来来说，这个显而易见就不是那么好定义，为了处理这个“显而易见”，MCTS的方式是对于当前选点，随机选取n个可能对局，以胜的概率来表示这个点的好坏。 对于第一个问题，如何找到可用选点呢，有经验的人类棋手凭借“棋感”直接找到直接能够找到几个看起来不错的点，机器却没有棋感，对于这点，机器通过随机生成多次对局，在每次对局中被访问到的次数比较多的节点，被认为是较优的选点。 MCTS实现那么在实现MCTS过程中，如何对外暴露接口和构造搜索树呢？ 我们以五子棋为例，探讨这一过程。 1234567891011121314class MCTS(): # ... def play(self, row:int, column:int, board_state:list): '''AI exposed interface''' board = Board(row, column) board.set_state(board_state) for n in range(self.playout_num): board_copy = copy.deepcopy(board) self._playout(board_copy) move = max(self.root.children.items(), key=lambda a: a[1].visited_num)[0] x, y = board.interger_to_coordinate(move) return x, y MCTS对外暴露的接口是拿到一个局面状态之后，返回推荐选点。 在这一步中，MCTS通过模拟一定次数的对局，以当前节点为根节点构造搜索树，然后查找搜索树中访问次数最多的二代子节点即可。 在每次对局当中，我们需要进行节点选择和评估，节点选择也是构造整个搜索树的过程，每次对局如果没有结束，就会以某一形式扩展当前节点。 在这个naive的实现当中，我们是直接扩展所有没有被棋子占据的节点。 12345678910111213141516class MCTS(): # ... def _playout(self, board: Board): node = self.root while(True): if node.is_leaf(): break action, node = self._select_best(node) board.move(action) action_probs, _ = self._policy(board) end, winner = board.who_win() if not end: self._expand(node, action_probs) leaf_value = self._evaluate_rollout(board) self._update_recursive(node, -leaf_value) 当扩展完当前节点之后，如果该到该节点游戏没有结束，为了避免无穷尽地递归搜索，需要评判该节点局面好坏，这部分通过在该节点内随机选点下棋实现。 123456789101112131415class MCTS(): # ... def _evaluate_rollout(self, board, limit=100): '''return -1 or 0 or 1''' player = board.get_cur_player() for i in range(limit): end, winner = board.who_win() if end: break action_probs = self._rollout_policy(board) max_action = max(action_probs, key=itemgetter(1))[0] board.move(max_action) else: print(&quot;tuoshi&quot;) return winner 如此我们实现了基本MCTS的功能，即给定局面，返回推荐值。 总结一下，在这个过程中，关键是构造搜索树，通过模拟在当前状态之后的多次对局实现。在模拟的过程当中，一边添加叶节点，一边通过随机对局更改叶节点和祖先节点的评估值，每个叶节点对应一个棋盘状态。当多次对局完成之后，选择搜索树中二代节点评价最高的节点。 总结MCTS并不难，基本思想就是通过多次随机模拟对局实现局面评估，从而降低将所有叶节点展开带来的巨大时空复杂度。 但是有个坏处，每次对局重新开始后，都需要通过模拟进行判断，过去下的每盘棋的经验无法被保留。自2006年MCTS被用来作为棋类求解算法以来，对于MCTS有许多基于特定游戏规则的选择策略改进，但都只是实现了人类知识的迁移，而没有自我改进这一过程，而2015年alphago横空出世，将神经网络算法引入围棋游戏，利用神经网络判定局势，给围棋这一古老游戏带来了新的变化。","link":"/2021/02/14/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95/"},{"title":"Octo INT8 training","text":"Octo: INT8 Training with Loss-aware Compensation and Backward Quantization for Tiny On-device Learning (ATC 2021) 背景：系统，单设备，边缘+训练+量化。思路亮点：引入LAC+PRC。系统亮点：低依赖，只需python基本库。 研究背景边缘端高效模型训练是实现边缘智能的关键环节。已有研究对边缘端数据收集和推理加速进行了深入探讨，其中量化（即将神经网络模型中的参数权重从FP32转为低精度格式数据，从而实现存储和计算开销降低的方法）是数据中心实现高效模型推理和训练的一种有效方案。 研究问题但是目前将已有的模型量化方法应用到轻量级设备模型训练存在四个问题 部分研究工作适用于推理场景，不适用模型训练； 部分研究工作只适用特殊网络结构，因为采用了极低比特设计（比如4bit存储）； 不能够在训练阶段实现硬件层面实际加速（理论可以，实际不行）； 没有对训练阶段算法做充分优化，无法适应低资源设备。 总之，已有的模型量化方法可适用在边缘端推理，或者理论上可行，但是实际不适用边缘端模型训练加速。因此要设计一种能够实际应用在边缘侧的模型训练的量化加速方案。 系统设计中的挑战和思路 如何充分利用硬件算力以及保证在不同边缘的通用性？（设计） 将统一的8位量化引入卷积运算、仿射模块、激活函数和梯度计算。数据量化涵盖前向和后向传递，因此可以加速整个迭代。卷积模块、仿射模块均基于numpy库编写，确保通用性。 量化会降低前向计算和后向计算的精度，如何保证精度降低范围可接受？（精度） 通过整体调整前向和后向传递的中间结果来保持模型的准确性。在前向传递中，提出了损失感知补偿(LAC)方法并设计了一个新的神经网络组件，称为补偿层，以填补量化张量算法引起的误差差距。补偿层内部的参数根据网络损失进行优化，为了提高更新效率，引入了补偿参数的L2正则化项。在反向传播中，提出了参数化范围裁剪(PRC)方法来限制量化梯度的变换域。 引入新的层是否会带来额外计算开销，从而抵消量化带来的性能优化？（系统开销） 以INT8格式保留所有参数和中间导数。可以有效地减少了峰值内存占用并节省了访问参数缓存的存储成本。对于上一个问题中提到的LAC和PRC可能会产生额外的开销，将补偿项从使用FP32张量的更高次多项式转换为仅依赖于卷积层输出的仿射运算，从而限制补偿和裁剪的计算成本。 前向LAC层和后向PRC方法量化误差（补偿）表达式： 补偿层设计代码： 123456789101112131415class Compensation: def __init__(self, alpha, mu, offset, enable_compensation_clipping=False, layer_id='Compensation'): ... def forward(self, x): out = x + self.alpha * self.mu + self.offset self.fp_out_before_clipping = out if self.enable_compensation_clipping: out = self.parametrized_range_clipping(out) return out def backward(self, dout): ... 含有误差$L_2$范数正则项的损失函数 在执行反向传播过程中，最终优化的目标函数变更为上式。 在后向传播过程当中，也可以使用梯度量化的方法，因为梯度也可以抽象为向量点积，遵循链式法则。但是后向传播计算梯度过程不能通过添加补偿层的方法降低量化误差。为了降低梯度的量化误差，关键在于降低零点偏移，通过对具有不同数值分布的张量确定不同量化范围，从而使零点偏移降低。并根据张量的数值分布通常呈钟形这一特点，启发性地将量化范围缩小为原来的0.95。 123456789def parametrized_range_clipping(self, tensor, z=2.576): n = tensor.size if n == 0: return tensor self.clip_max = 0.95 * np.max(tensor) self.clip_min = 0.95 * np.min(tensor) return np.clip(tensor, self.clip_min, self.clip_max) 复现和思考 由于基于numpy库编写，能够实现不同边缘平台的推理和训练，通用性可以保证。 实验数据集为MNIST，CIFAR10小数据集，完成任务是图片分类，属于比较简单的机器学习任务。代码支持的算子主要是卷积、仿射和部分激活函数。比较的基线是基于其自身框架实现的量化算法。可以考虑根据该库添加不同算子，实现不同模型，或者进一步改进其性能。 目前能够在GPU和NPU上顺利运行，对于不同平台支持的优化方案比较还需要进一步验证。","link":"/2021/09/29/Octo-INT8-training/"},{"title":"MEC中的任务卸载和资源管理研究","text":"北邮田辉2019年毕业学生的博士论文，题目是移动边缘计算中的任务卸载和资源管理研究。 论文结构论文整体研究对象如标题所阐述。 第二章讨论动态移动边缘计算环境中用户任务迁移问题，分别针对单用户多边缘和多用户协作场景进行优化，并通过时域滚动控制和Lyapunov优化技术设计在线优化方案，减少系统能耗，提升系统吞吐量 第三章研究MEC单小区（一个边缘服务器）资源管理问题。分别针对普通移动应用、时延敏感任务，以及海量设备连接物联网场景进行优化。具体地，针对普通移动应用，提出了均衡任务计算时延与处理能耗的效用函数，并结合凸优化、准凸优化和次模优化技术，对任务卸载决策、无线资源管理和计算资源分配进行联合优化；针对时延敏感任务，提出了基于量化动态规划技术的接入控制和资源分配联合优化方案；针对海量设备连接物联网场景，提出了基于加扰Lyapunov优化技术的物联网边缘计算在线资源调度方案，最大化系统效用。 第四章研究了移动边缘计算服务器分布式组网资源管理问题，分别针对大规模边缘计算和分布式机器学习的边缘计算组网资源管理进行优化。具体地，针对大规模边缘计算组网资源管理，提出了大规模边缘计算资源管理和协作域划分的分布式优化方案，在未知网络随机特性的情况下最小化时间平均系统开销，并通过协作域限制任务卸载范围，保证大规模网络边缘计算性能；针对分布式机器学习的边缘计算组网资源管理，设计了针对分布式机器学习的数据均匀度指标，并提出了数据接入、分配和处理在线优化方案，在保证网络稳定条件下，最大化分布式机器学习系统效益。 动态移动边缘计算环境中的用户任务迁移问题待续","link":"/2021/10/05/MEC%E4%B8%AD%E7%9A%84%E4%BB%BB%E5%8A%A1%E5%8D%B8%E8%BD%BD%E5%92%8C%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E7%A0%94%E7%A9%B6/"},{"title":"nn-Meter","text":"nn-Meter: Towards Accurate Latency Prediction of Deep-Learning Model Inference on Diverse Edge Devices (Mobisys 2021 Best paper) 背景：理论，单设备，边缘+推理+推理时间预测。思路亮点：找融合算子，测融合算子运行时间，指出算子计算时延和算子配置存在非线性关系。 研究背景边缘端实现高效模型推理是实现边缘智能的关键环节。推理延迟成为各类移动端和边缘设备上衡量DNN推理模型是否高效的关键指标，如何准确预测推理延迟是设计各类高效神经网络的关键问题。 研究问题但是目前预测神经网络模型推理延迟存在以下几个问题： 对于不同的边缘设备（例如，移动CPU/GPU和各种AI加速器）和不同的推理框架（例如，TFLite和OpenVINO），工程量是巨大的。即使在单个设备上，测量NAS任务中的大量模型也可能非常耗时，在实际工作中基于测量的方法不可行。 基于FLOPs的预测方法被广泛应用，这是一种简单的方法，但并不是延迟的直接度量，因为底层硬件可能存在的各类并行优化使得此类方法不准确。 基于神经网络基本操作符的推理延迟预测不考虑计算图的运行时操作符融合优化，从而导致模型延迟差异。 基于图形卷积网络（GCN）预测各种设备上NASBench201数据集的延迟是最近出现的工作，但是这种基于模型图的方法很大程度上依赖于已测试的模型结构，可能不适用于许多看不见的模型结构。 总之，已有工作或者在实际中不可行，或者没有考虑底层各类运行时优化，或者对未知模型缺乏预测理论依据，因此需要建立一种适用于边缘设备底层算子融合优化，具有理论依据的推理延迟准确预测工具。 算法设计的思路和挑战下图系统框架展示了实现DNN模型精确延迟预测的两个核心组件：内核检测和自适应数据采样。从概念上讲，前者自动将目标模型划分为一组内核，后者从大空间中采样最有利的配置，以构建准确的内核级延迟预测器。然后，对于每个内核，我们提取特征并预测延迟。 内核检测。 它包括精心设计的测试用例来检测两个算子之间的融合规则（融合后即内核），以及一个搜索模型中所有核的算法。通过离线收集所有的融合规则，对于在线模型预测，核搜索算法递归地将这些规则应用于目标模型以查找所有核。 寻找包含融合规则的核有两个难点：许多推理后端是闭源代码，无法从源代码中获得内核；另外CNN模型任意，为了支持模型推理时间预测，检测融合规则方法应该独立于特定模型图。 自适应数据采样。 受限离线为目标设备上的所有内核构建基于机器学习的时延预测器。对于每个内核，它通过一个迭代的采样过程来采样最有益的配置(h,w,cin,cout,stride,k)。采样器从先验可能性分布中采样，该分布描述了模型CNN设计中考虑的内核配置。我们设计了一个测试集来评估采样数据的质量，对于预测误差较大的数据，在其周围执行细粒度通道数采样。 造成误差较大的原因是部分操作符的推理时延和一些参数并不是简单的线性模式，而是阶跃式函数，这种非线性反映了硬件优化的复杂性。 融合规则检测判据和算子配置方法由于融合通过将同一元素上的计算连接在一起，从而减少了延迟，因此使用连接和分离运算符的运行时间差作为判断是否发生融合的度量。也就是说，对于两个操作符op1和op2，如果操作符的时间遵循下式，则它们被视为被融合为Op1++Op2。 然后从计算图的根节点遍历整张图，依次根据此判据判定不同算符是否有融合规则。 如以上两张图所示，如果直接随机采样会忽略许多关键数据，拟合得到的预测结果不准确。由于算子配置在样本空间中呈现非均匀分布（一些极端情况的采样实际不会用到），首先采用剪枝将这类情况排除，然后运行一个迭代过程来围绕不准确的预测数据采样得到更多的数据，直到预测精度满足要求。对于非线性数据关系，采用随机森林为每个内核构建推理实验预测器。 最终所有内核的预测推理时延求和就是一个模型的推理时延。 复现和思考没有复现，原因有二：首先对一个模型进行精准预测，和调度关系不是很大，可以作为调度的支撑，但不是主线；其次代码依赖较多，复现需要耗费时间较长。 另外这篇文章首先扩大问题规模说明推理延迟预测算法的必要性，然后在方法阶段提出用剪枝剪去现实中不存在的可能性，有些离谱。 不过收集数据集不容易，工作量还是有的。","link":"/2021/10/10/nn-Meter/"}],"tags":[{"name":"随笔","slug":"随笔","link":"/tags/%E9%9A%8F%E7%AC%94/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"论文阅读","slug":"论文阅读","link":"/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"}],"categories":[]}