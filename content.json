{"pages":[],"posts":[{"title":"Hello World","text":"2021年1月26日 星期二 看到阿伟博客写的不亦乐乎，在复习计算复杂性时受益良多。本人对此心生神往，于是在2021年寒假空闲时简单搭建此博客，用以对一些零散的感悟加以总结","link":"/2021/01/26/hello-world/"},{"title":"前20周","text":"一个迟到的总结。 1月17号周日，刚好是开学后第二十周的结尾，18号，也就是第21周的开头，回到东北开启度假模式。 来南京之前，只定了两个目标，保持身体健康和发论文毕业。来到南京这140天，身体素质大概是保持住了，可惜在达成毕业条件这个事情上，有太多的诱惑使我不能保持持续的专注： 两门课分散了精力，计算复杂性和并行算法 认识了一群有趣的伙伴 围棋和拍照 研究方向分散，啥都想研究一下 关于课程计算复杂性对于第一次上这课的印象，我只记得双手支起脑袋，努力瞪大眼睛，听到什么图灵机的符号转移之后，大概就昏昏欲睡了，然后就是下课喧闹的声音。 第一堂课确实劝退，不过我却知道了一个事实：林老师脾气不错，让我这混学分的看到了希望。同时自己心底也抱有这样的想法：毕竟也是学计算机的，要是连计算机能处理什么问题，不能处理什么问题都不能有个大概的了解，岂不是白学了？ 看得出林老师非常明白自己要讲的东西，可惜这课确实不太简单，菜鸡如我用一堂课的时间理解什么是BPP复杂度类，什么是PH复杂度类，着实不太容易。就算是对于NP问题定义里的字符串x，语言L和certificate，尽管自己能理解这个是什么意思，可对应到具体的数学语言上，我只想说不管怎么理解都不太对。转折大概是13周吧，在问了老师和徐哥很多关于概念，看起来非常幼稚的问题之后，竟发现自己不仅这些概念弄通了，连带着作业题也会做了，最后的考试林老师也非常给力，7道题都不难，顺利通关。 尽管和完成毕业论文要求的这一目标关系不大，这门课听进去还是蛮有意思的，前辈们关于什么能够计算，什么不能够计算，计算的代价等诸多问题理性而曲折的讨论，确实是计算机领域最为引人入胜的话题。 并行计算诶，一门早八的课，每次上课都难受的要死，老师讲课逻辑还挺清楚，可音调之平淡，让我一个早上犯困的人认真听100分钟，着实艰难。除此之外，这课主要关注的是并发算法的可行性，老师的博士毕业论文是关于可线性化点的（Linearizable point），找这个点可是扎扎实实的NP难问题，全靠对并发数据结构的熟悉和骤然一闪的灵感。可怜我最后竟坚持上完这课到考试，不过这本书《the art of multiprocessing programming》写的倒是有趣，其中一段关于乐观锁的比喻至今还记得。 大意是在乐观锁实现里面，每个线程可以比喻成一辆不同寻常的出租车，怎么不寻常呢？一次有位乘客坐上了这个出租，这出租从来不在红灯处停车，开的飞快无比，突然间这出租在一个绿灯处停了，乘客人傻了，问这司机是不是脑子有泡，司机告诉他：“我是怕也有像我一样的司机，一直闯红灯。” 关于这门课考试，随堂考，大概八开纸8~9页，考试那天俺还迟到5分钟，第一面送分题用了大概40分钟。艹（一种植物），想到俺还有一次作业没交（总成绩10分），心里拔凉拔凉的，还好最后算是过了。当时为啥想不开要上这门课呢，这门课大概就是自觉学得还行，可是出题比较难的那种吧。 其他至于本学期其他课程，花费精力不多：矩阵论出题非常给力，半小时交卷走人（后面那哥们一直抖腿，加上想上厕所，要不也不会这么快）；分布式系统助教师兄给分良心，Raft实验还挺有趣；物联网导论我还真没去过；嗷，对了，值得一提的还有窦老师的前沿课，金句频出，什么凌晨四点接到陈老师（可能是窦老师那个组的负责人）的微信，请窦老师点评自己的一段发言稿录音，窦老师表示只有四个字能评价：“天籁之音”，再加上窦老师那副非常真挚的面容，回想起来非常有趣。 关于课程大概就是这样了，大概以后都不会再认真上一门像计算复杂性那样的课了诶。 关于娱乐开学初去了趟围棋社，果然江苏的下棋氛围还是浓厚，本科时的朋友没有骗我。小小围棋社竟有50人余，围棋也不是只属于棋牌协会一个部门，而是单独社团，想我本科时候那得叫棋牌协会围棋部。社团内部高手也不少，随便拉个人都是5段，不知道是5段贬值了还是我变菜了hhh 浏览器主页上的第一个收藏网站就是弈客围棋，github排在第三个，有时候想去github上找点东西，随手就会打开弈客围棋，以至于观棋或是下棋忘了时间。就算在实验室，感觉后面师兄总在看着我，手也会像具有惯性似的这样做，希望在主要目标完成以前，能够适当控制自己这方面的爱好诶。 至于摄影呢，投入精力不多，但是寻找拍摄地点画的时间也不少，因为摄影也算是去了不少地方，算是比较消耗时间的爱好，之后转过去玩玩人像摄影，权当多接触人吧。 剩下的什么三国杀，狼人杀等各种桌游，还有炉石等游戏，投入的时间也不少，从10周以后，大概每周都会有1到2次。实验室的朋友们非常浪，总带我进行这类活动，下一阶段为了发论文这一最高目标，这类活动怕是要先停一停喽。 关于研究这学期最开始，认识人不多，也没有课，那真是狂读论文的日子，实验室只有我和后面三位师兄常来，最开始读的论文是infocom上的，那个理论和公式是真的多，一整7、8行的优化表达式，有些还不一定对。后来这类论文读了3篇左右，读的俺是头晕眼花，不过也算摸清这类论文的套路了。之后又开始看一些偏系统的论文，sigcomm上的居多，这论文读起来有很多东西需要查资料，可公式确实少，尽管长，可好读一些。后来又去蹭组会听，胜哥的，翘课听了组里博士讨论班，lamda组老师的，周sir的。大部分老师和和气气，不过大多思维非常敏锐，问题很犀利。 印象最深是周sir组会，组会之前有个空桌，上有精致茶具沏好的一壶茶，时间一到，周sir准时赶到，待得学生报告完之后，周sir得着自己学生先挨个提问，最后总结，语言精炼，不算很长，可总结完之后必定是要喝完一杯茶，并再高高地抬起茶壶沏满一杯，非常潇洒。 不久课程开始忙碌起来，老钱给我和zyt弄了个项目，做那个项目去嘞，这个项目可真是个坑，项目方的意思是糊弄个文档，而老钱意思是让我们挖掘问题，可这项目就是K8S套了层壳，找个锤子问题。项目方的场景是将docker容器当虚拟机使，赋予容器更强大的能力，可这就是个伪需求（画外音：那俺直接用虚拟机不行，要啥docker）。大概只有国企的领导脑子进水才会想到这种用法，而只有应付国企的z国特色乙方企业才会接这种需求。只有处于产学研（研究生生产学习）一线的俺们才会研究这个东西，好在老钱还挺够意思，年底发了些补助慰问我们，并且让我们暂停了这个东西。 项目持续大概两个半月，每周都要做一些实验、文档和汇报，分散了我的一部分精力，加上那段时间还各种浪，基本属于研究停滞的状态。这个项目让我看透了一些东西，见微知著，我大概知道z国的一些科创项目（或者说大部分？）具体是怎么回事，其实最开始还是有些失落。当然，路毕竟是人走出来的，希望还是要有的嘛。 吸取这段经历的经验，我决定先搞个跑实验比较简单，容易出活的。想想学期最开始联邦学习看的也不少，这个实验也好做，就先从这个来了，回家乡这几天主要就在干这事，当然，这些就是第20周之后的事了，休假结束再总结吧。 关于一些有趣的人和事毕竟不是日记hhh，就不具体记录了，只能说每个人的想法真是丝毫不同，尊重并尽量理解不同个体的表现和选择吧。 有些时候在一个方向研究久了，比如俺自己，计算机这方面的书看多了，不看其他方向的书，确实容易形成思维定势。想我从初中开始看的书逐渐偏向理工科，本科时看的书更少，这个假期的空闲时间是该狠狠读一波书了。","link":"/2021/01/26/%E5%89%8D20%E5%91%A8/"},{"title":"新篇章","text":"2022年7月末到2022年年末，有小半年（21周）的时间没有更新博客了。正好最近心怡和俺都偶感风寒，闲下来记录一下。 正文2022年下半年过的很快，也太快乐。这一年有些想法也在逐渐转变，未来的计划也要有所转变。 第一是我遇到我想与之共度一生的人，我们很契合，我想娶她；第二是现在的经济状况变得比较宽裕，静资产第一次超过10万，家里给打了钱基本没要，主要是做项目、博士工资、奖励和一些其他收入；第三是在课题组内承担了更多工作，带了挺多本科生，很多项目现在是我来牵头带组里的其他博士搞；第四是疫情发展和科技发展交织，机遇变多。 因此未来要做出对应变化： 更重视和家人、爱人和亲人的关系。其他人没有太多精力关注，舍友是舍友，群友是群友，牌友是牌友，工作伙伴是工作伙伴，实验室同学就是同学，老板就是老板，带的学生就是学生。不用太过在意维护和经营这些人的关系，需要你的时候他们能想起来，能最终长久交往的人太少。 重视长远目标：短期内评个奖学金或者发一篇论文这些对我来说已经没有太大帮助和新鲜感了。我更要聚焦在未来道路的规划和实践上。赚钱和更优质的资源是所有人的共性追求，自己的兴趣是理论性质较强、并且能看到落地的方向，但是这种追求太理想化，不适合我将它和共性追求结合在一起，兴趣有必要对共性追求让步，在实践中逐步找到这样的结合点。 实践比想法重要：想到就做，做的过程中调整方向，最后要有成果。我比较喜欢思考规划和总结，因为这个原因有时候也容易受他人行动、观点和其他信息流冲击。现实情况是大部分东西做了就有收获。做起来也不容易被干扰。务实！ 2023年的一些重要时间节点： iwqos截稿日2.06； secon截稿日5月中旬； 中期答辩7月； 4月云南自驾； 6月毕业旅行； 预计完成目标：iwqos或者secon中一篇即可，通过中期答辩。自驾争取不违章。下半年准备冲A。 除了这些任务，最重要的还是希望自己和所爱的人都健康平安快乐。","link":"/2022/12/29/%E6%96%B0%E7%AF%87%E7%AB%A0/"},{"title":"置身事外：学校教育和社会政策间的鸿沟","text":"在21世纪初的社会，一个人刚学着独立生活的时候，学校教育塑造了他的认知。 然而伴随着独立生活的需求，他与社会的方方面面产生了联系。当他用十几年来学校塑造的旧有认知试图解释社会运转的现象时，“如此生活三十年，直到大厦崩塌”之感油然而生。 这时，他才会发现真实社会的运转规律和在学校形成的认知之间存在着一条难以逾越的鸿沟。 为了生存，为了解惑，为了一个明明白白的人生，一个新的理论，新的行动指南有必要被建立起来。 瞠目结舌的社会现象我们接收到的信息，有很多“离谱”的现象：为什么一个号称代表人民的政府出台的政策与民意相违背？为什么一些奇奇怪怪的癖好能被拍成电影并被传播？为什么一生勤勉劳作的人到最后无人问津？为什么一个漏洞百出的骗局却有人相信？为什么一个对社会发展毫无进步的内卷式体系可以存在？…… 难道是这个社会就是有些人非蠢即坏，不可理喻，生下来就要恶心别人或者被别人恶心吗？ 人的认知人的本性来自于灵长类动物的基因记忆，来自于后天每一阶段接触的人或者事物，这些共同塑造了一个人在面对社会实践中的行事方法。有的人把这种行事方法称为一个人的三观：人生观、价值观、世界观。这种把社会实践中的行事方法粗糙的分为三个方面的观念，恐怕是不太合适的，但由于没有其他比较好的替代，姑且用“认知”这个词来描述。 人的认知具有哪些特点呢？如果用深度强化学习这样的学术方法描述智能体的话，大概会是一个深度神经网络，辅之以大量数据进行训练，最后得到一个权重参数，抽象的语言难以描述。今天我们也用统计的方式，不过是用抽象的方式，给我这些年的经验做一个总结。这个总结，只是概述的，而且会随时发生改变。 “三观”具有动物属性：来自于基因的传承记忆，一个人会有与生俱来的繁衍冲动、会有使自己变强活下去的生存本能、会有害怕来自未知的恐惧本能、会有面对危险时的急中生智，也会有爱和被爱的协作能力。 “三观”具有灌输属性：来自于“三观”未成时期家庭和学校的漫长哺育期，注意学校这个概念仅在我们当前这个社会存在，未来是否会有学校我们在这里不讨论。由于此时一个人尚处于“一张白纸”的阶段，信息的单方面灌输可以短暂地塑造“三观”。 “三观”具有实践属性：来自于个体将“三观”应用于社会实践中时，通过接受实践反馈，调整其行为策略。 “三观”具有集体性质：来自于他人对个体的影响，集体属性依赖于实践属性、灌输属性和动物属性。是个体“三观”向集体“三观”的推广。 “三观”具有时变性质：个体的三观并不一成不变，重大历史节点或者长期潜移默化都会对个体的行为产生影响，时变性来源于“三观”的实践属性。 回答：放弃学校教育先入为主形成的认知回到原先的问题：为什么我们看到那么多“离谱”的现象？如果人的“三观”的来源和形成遵循我们谈到的几点经验性质的规律，我们又承认这些“离谱”现象确实已经发生，那么我们将只能做以下推论： 推论1：“三观”的动物属性与我们是相同的，灌输属性造成的差别可能是有一定不同的，实践属性造成的差别一定是巨大的。 推论2：由推论1可知，“离谱”现象涉及到的人或者群体，其所处的社会实践和我们发生的巨大的偏差。 换言之，如果“离谱”现象发生，我们应该认为有我们所不知道的另外一种运转规律，并根据现象，猜测和分析另外的社会环境，由此推动认知的迭代升级。 验证“离谱”现象背后的特别社会环境：即使某一群体所处的社会实践和我们有巨大的差别，但“三观”的属性和性质没有改变，动物属性仍然相同，他们的社会实践产生的新“三观”仍应该符合“三观”的特性，通过这一点来确定认知是自洽的。","link":"/2023/08/12/%E7%BD%AE%E8%BA%AB%E4%BA%8B%E5%A4%96/"},{"title":"总结：2023-2024.06","text":"2023-2024.6 正文2023年初《新篇章》设定的时间节点目标已经完成。里面提到的三个变化（关系、目标、行为模式）目前在逐步确立。 三个变化 更重视和家人、爱人和亲人的关系。其他人没有太多精力关注，舍友是舍友，群友是群友，牌友是牌友，工作伙伴是工作伙伴，实验室同学就是同学，老板就是老板，带的学生就是学生。不用太过在意维护和经营这些人的关系，需要你的时候他们能想起来，能最终长久交往的人太少。 关系模式发生好的改变，目前很满意。未来和这些人的关系需要以组织成员的角度来看待。（《组织的形成》） 重视长远目标：短期内评个奖学金或者发一篇论文这些对我来说已经没有太大帮助和新鲜感了。我更要聚焦在未来道路的规划和实践上。赚钱和更优质的资源是所有人的共性追求，自己的兴趣是理论性质较强、并且能看到落地的方向，但是这种追求太理想化，不适合我将它和共性追求结合在一起，兴趣有必要对共性追求让步，在实践中逐步找到这样的结合点。 目前有一个大致方向（一龙马），2023下半年和2024上半年对很多东西有尝试（自驾、地理、滑雪、产业链分析、预测优化、强化学习、斗争策略、吹牛、国内特色的酒局和送礼文化、reuse、boxing），仍然比较散，总体来说为这个方向提供了一些认知基础，但没有进行认知实践。未来需要对已有能力进行梳理整合，并形成符合实际的规划。（《认知的实践》） 实践比想法重要：想到就做，做的过程中调整方向，最后要有成果。我比较喜欢思考规划和总结，因为这个原因有时候也容易受他人行动、观点和其他信息流冲击。现实情况是大部分东西做了就有收获。做起来也不容易被干扰。务实！ 认知和行动指南的框架已基本确立。未来对于组织的认知还需要迭代。（《认知的迭代》《认知的实践》《实践和集体属性》《认知的传播来源和集体属性》《组织的形成》《置身事外：学校教育和社会政策间的鸿沟》） 时间点接下来到2024年末有如下大时间节点： 8.1 infocom截稿 8.15 AAAI截稿 9.1 TPDS和TMC 6月末国道333 10.1 澳洲 12月俄罗斯 目标是期刊一篇，会议一篇。 除了这些任务，最重要的是希望自己和所爱的人都健康平安快乐。 附1：认知和实践的关系123456789101112131415161718192021222324252627认知└── 来源 ├── 动物：身、心、脑 ├── 实践：效率低 但可验证 ├── 传播：不保真└── 属性 ├── 集体 ├── 时变└── 目的：（马斯洛需求层次） ├── 生存安全 ├── 明白（不被欺骗） ├── 自我实现实践└── 目的 ├── 提升认知（辅） ├── 自我实现（主）└── 难度 └── 高效└── 方法 └── 局部能力 ├── 情绪控制：情绪-行为分离 ├── 决策控制：降低决策开销 ├── 行为控制：身心脑合一 └── 全局能力 ├── 梳理整合（过去认知的总结）：引入外存 ├── 部署下一步（未来实践的规划）：避免过细和过粗的谋划 参考文献：《认知的迭代》《认知的实践》 附2：组织1234567891011121314151617181920组织└── 源起 ├── 认知的集体属性 ├── 人类生存的现实需求└── 分工 ├── 决策：能给组织带来增量 ├── 执行 ├── 中间层└── 兴衰 ├── 形成：共同纲领 ├── 维系：产生增量└── 衍变 ├── 分裂：组织可以构成规模更大的组织 ├── 斗争：内部 外部 ├── 合作：增量方向一致└── 个体参与组织的实践 ├── 理念 ├── 定位 ├── 影响（认知） ├── 退出 参考文献：《实践和集体属性》《认知的传播来源和集体属性》《组织的形成》《置身事外：学校教育和社会政策间的鸿沟》","link":"/2024/06/07/2023%E5%B9%B4-2024%E5%B9%B4%E5%85%AD%E6%9C%88%E6%80%BB%E7%BB%93/"},{"title":"实践和集体属性","text":"个体的共同纲领形成了组织。又因为个体的认知差异和实践水平在组织内形成不同分工。 组织可以分为决策和执行，庞大的组织需要中间层实现决策到执行的传递，中间层的出现会扭曲决策的本来意图。 组织的决策就是大多数个体认知在组织中的体现，组织的执行水平就是大多数个体实践水平在组织中的体现。","link":"/2024/04/01/%E5%AE%9E%E8%B7%B5%E5%92%8C%E9%9B%86%E4%BD%93%E5%B1%9E%E6%80%A7/"},{"title":"组织的形成","text":"共同的纲领团结个体形成组织。沿着共同纲领持续产生增量的组织才能够维持。 群体对资源的获取、分配和使用体现了组织发展衍变的过程。组织制度约定了群体内资源分配环节的规则。 把视野聚焦到社会这个特定的组织类型，我们可以做如下判断： 社会制度是社会发展的指挥棒和引擎，它通过规定资源分配方式，影响资源的获取和使用效率，从而驱动社会的发展和社会整体财富的变化。 评价一个社会制度对于社会稳定的维系作用，最重要的是要判断它能否指挥群体的资源获取和使用效率进行改善和提升. 正文基本观察1：在群体中，当个体对于资源使用效率的提升做出了正面贡献，其可以得到分配上的正反馈，这样的分配制度会带来社会财富的增量。 基本观察2：群体中个体分化特别明显，大量个体为了基本的生存保障而奔波时，这样的社会难以取得增量。 基本观察3：个体的行为受到严格限制，社会观念不包容变化时，个体陷入僵化的社会很难取得增量。 不完全归纳的推论：群体增量的取得在于个体是否具有主观能动性，是否愿意求新求变，是否具有客观的创新条件。 中国历史阶段资源分配方式对于资源获取使用效率的影响 秦朝：奖励耕战推动农业和军事发展，在一定程度上扩大了农业生产和战争的产值，为秦灭六国提供了物质基础。 汉朝：实行推恩令，鼓励农业生产，分封制度的实行也在一定程度上保证了地方王室贵族的利益，使其400年不倒并扩大疆域。 宋朝：科举制度完善，商业税收改革，鼓励商业和贸易，但是后期程朱理学极大地束缚压制了人的观念。 明朝：严格的官僚制度，使得社会的创新力受到限制，资源获取的效率也相应受到影响。 清朝：重农轻商，闭关锁国，极少产生新的财富增量，财富向统治阶级集中，底层人的生活极度压抑。 这些王朝或多或少吸收了前朝灭亡的历史教训，在王朝建立的最初阶段，推行均田制，避免大地主产生，但是王朝后期不可避免地产生了土地兼并等现象。从而导致贫富差距扩大，底层个体难以通过新的增量获得财富，而统治阶级不愿意出让既有利益，终究难逃暴力革命的历史宿命。明、清的统治阶级相对于前代，用程朱理学团结统治阶级内部，分化被统治的底层民众，用基于程朱理学的科举制给底层一丝机会，进而瓦解底层的组织，然而程朱理学目标在维稳而不是产生新增量，随着时间的推移，统治阶级吸收了王朝的大部分财富，但是王朝整体却相对更加衰微，最终当文化碰撞时，彻底崩溃。 增量的判断回到原先的问题：什么是增量？增量就是社会总体财富随时间的变化值。那么如何衡量社会总体财富呢？一个时期内利用资源进行生产活动的总量，也就是GDP，这是一个很好的用来统计社会产生总财富的指标。只要分配方式能够使得GDP持续增长，社会就呈现为增量型社会，即使分配环节有一些不公，社会就还能维系。 由于GDP衡量的是生产活动的总量，因此产业和GDP是密切相关的，换言之，产业做大做强，产业品种门类的丰富，也就意味着社会财富的增加。有些时候，即使拿不到宏观统计数据，或者宏观统计数据造假，只要能够直观地感受到相关产业的变化，我们就可以见微知著，判断社会是否产生新的增量。 历史学家根据对工具的使用将人类的发展划分为石器时代、农业时代、工业时代、信息时代。但是发展是过程，而不是在某个节点上的突变。每个时代都意味着大量新产业的不断成熟，社会财富积累的方式发生变化，从而进入新的增长阶段。随着产业的兴起，掌握新生产资料的阶级登上历史舞台，分配关系也随之发生改变。 在石器时代，人类的主要”产业”是狩猎和采集，为了温饱而斗争，增速缓慢。 几千年后，随着火、青铜器的使用，人类积累财富的方式从狩猎转化为开垦，从此进入农业时代，相继出现了陶瓷制造、冶金和金属加工、纺织和织布、建筑、食品加工、船舶车辆制造、养殖等产业，不过这时期的产业依赖手工，人力资源密集，然而食物的生产和积累速度并不足以满足这些产业的人力需求，因此这些产业相对于农业的产值并不大。 又是几百年的积累，随着这些产业的生产效率进一步提升，从事相关产业的新兴阶级开始从谋求经济利益转向谋求政治地位，一个经典的案例是岛国英国，纺织业的工厂主联合其他产业的新兴阶级，通过暴力革命使封建地主让步，从而获得了政治地位，可以合法地保留自己的财富，从而将农业社会对于土地开垦的增量依赖转变为对市场、原材料驱动的增量依赖，由于暴力革命也联合了新兴技术工人阶级，技术也成为了产生增量的新因素，从而引发了能源动力的革新。 这一阶段，人类积累财富的方式从农业开垦逐渐转变为多元化的积累方式，能源、技术、各种原材料，都能带来增量，产生了大量新的产业，纺织、矿山、冶金、机器制造、汽车制造、飞机制造等制造业；铁路运输、汽车运输、航空航海等运输行业；煤炭、石油、天然气、新能源等能源行业；化肥、塑料、药品、化妆品等化工业；住宅、工业建筑设计、商业等建筑业；电讯、广播、互联网等传播行业。 由于在各行各业中大量的计算需求，以及电子技术和产业的发展，依赖高精度的光学镜头、光源、光阻、纳米尺度的测控技术的高算力计算机出现，信息行业成为新的社会财富增长点：一方面，高质量信息对于个体本身是一种高级需求，另一方面，信息使得大量原有产业的生产效率得以提高，很多历史学家将这个时代归为信息时代。 我们难以说明信息时代和工业时代财富增长的本质区别，因为信息并不是一种必需品，但是信息时代带来了一个极具想象力的机器人产业。一个具有人类级别智能和运动能力的机器人的出现将大幅降低不同产业链上的人力成本，从而使得各类产业以几乎零成本的方式扩张。信息时代的产业链降本与具有机器人产业后的产业链降本其规模和程度不可同日而语，在当前的时间节点下，可以预期十年内机器人产业必然兴起。 我们梳理了人类发展历程中的不同阶段，但归根结底要具体到产业的变化，新产业的出现和壮大是过程而非节点。只有会出现新兴产业、产业规模扩大的社会，才会有财富积累，才会有增量出现，只有新的产业源源不断出现的社会，才可以避免崩溃和暴力革命。所以启示是什么呢？对于个人来说，选择能够壮大的新兴产业，选择鼓励新兴产业的社会制度；对于社会来说，拥抱变化，鼓励变化，给变化以机会，要让new money获得政治地位。","link":"/2023/11/10/%E6%9E%84%E5%BB%BA%E5%A2%9E%E9%87%8F%E5%9E%8B%E7%A4%BE%E4%BC%9A/"},{"title":"蒙特卡洛-树搜索方法","text":"棋类问题求解属于PSPACE-HARD，以围棋为例，解空间约有为$10^{250}$种可能。就算是超级计算机每秒万万亿次计算速度，即每秒能够穷举$10^{16}$种状态，对于这么大的解空间来说也只是杯水车薪。 所以如果有人说如果算力足够，这个问题暴力搜索也可以，那他显然对$10^{234}$倍的差距不了解。 正文为了解决这么大的解空间搜索问题，一种随机性的策略被提出来了——Mento Carlo Tree Search。蒙特卡洛树搜索是将蒙特卡洛方法中的随机性引入到搜索树当中。有关文献也将这棵搜索树称为博弈树，因为对于棋类游戏，对弈方的目标是正好相反的。 所以，MCTS算法本质上树的搜索算法，输入是当前局面状态，输出是可能的最优解。 如果你意图使用暴力搜索的方法，只需遍历这棵搜索树，穷举所有可能，思路自然非常简答，但是解空间规模不允许你这样做。MCTS在搜索这棵树的时候引入了随机性。 回想我们下棋的方式，通常是脑海中出现几个可用的选点，对比这些选点的对于局势的影响，选择其中最有利的选点。 对于暴力搜索有两个难点： 1. 如何找到可用选点？ 2. 如何确定这些选点的相对好坏？ 先讨论第二个问题，对于人类来讲，确定选点的好坏往往需要递归的进行这样的思考，直到人类可以“显而易见”看出某个局面的好坏。然而对于机器来来说，这个显而易见就不是那么好定义，为了处理这个“显而易见”，MCTS的方式是对于当前选点，随机选取n个可能对局，以胜的概率来表示这个点的好坏。 对于第一个问题，如何找到可用选点呢，有经验的人类棋手凭借“棋感”直接找到直接能够找到几个看起来不错的点，机器却没有棋感，对于这点，机器通过随机生成多次对局，在每次对局中被访问到的次数比较多的节点，被认为是较优的选点。 MCTS实现那么在实现MCTS过程中，如何对外暴露接口和构造搜索树呢？ 我们以五子棋为例，探讨这一过程。 1234567891011121314class MCTS(): # ... def play(self, row:int, column:int, board_state:list): '''AI exposed interface''' board = Board(row, column) board.set_state(board_state) for n in range(self.playout_num): board_copy = copy.deepcopy(board) self._playout(board_copy) move = max(self.root.children.items(), key=lambda a: a[1].visited_num)[0] x, y = board.interger_to_coordinate(move) return x, y MCTS对外暴露的接口是拿到一个局面状态之后，返回推荐选点。 在这一步中，MCTS通过模拟一定次数的对局，以当前节点为根节点构造搜索树，然后查找搜索树中访问次数最多的二代子节点即可。 在每次对局当中，我们需要进行节点选择和评估，节点选择也是构造整个搜索树的过程，每次对局如果没有结束，就会以某一形式扩展当前节点。 在这个naive的实现当中，我们是直接扩展所有没有被棋子占据的节点。 12345678910111213141516class MCTS(): # ... def _playout(self, board: Board): node = self.root while(True): if node.is_leaf(): break action, node = self._select_best(node) board.move(action) action_probs, _ = self._policy(board) end, winner = board.who_win() if not end: self._expand(node, action_probs) leaf_value = self._evaluate_rollout(board) self._update_recursive(node, -leaf_value) 当扩展完当前节点之后，如果该到该节点游戏没有结束，为了避免无穷尽地递归搜索，需要评判该节点局面好坏，这部分通过在该节点内随机选点下棋实现。 123456789101112131415class MCTS(): # ... def _evaluate_rollout(self, board, limit=100): '''return -1 or 0 or 1''' player = board.get_cur_player() for i in range(limit): end, winner = board.who_win() if end: break action_probs = self._rollout_policy(board) max_action = max(action_probs, key=itemgetter(1))[0] board.move(max_action) else: print(&quot;tuoshi&quot;) return winner 如此我们实现了基本MCTS的功能，即给定局面，返回推荐值。 总结一下，在这个过程中，关键是构造搜索树，通过模拟在当前状态之后的多次对局实现。在模拟的过程当中，一边添加叶节点，一边通过随机对局更改叶节点和祖先节点的评估值，每个叶节点对应一个棋盘状态。当多次对局完成之后，选择搜索树中二代节点评价最高的节点。","link":"/2021/02/14/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%A0%91%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95/"},{"title":"观后感","text":"一部电影2个小时，看完总要说点什么。 西游降魔篇：玄奘的信仰陈玄奘的信仰是真正的信仰。 他的信仰没有任何欲念作为回报，不断地割舍欲念反而让他的信仰更加真实和坚定。 村镇收取鱼精的经历割舍了他一丝虚荣的念头，猪刚烈的故事告诉他众生的魔念从不是水中浮萍，即使爱情对于他来说也只是一种怀念，而不是难以割舍的一部分。 所以当孙悟空打死玄奘所爱之人，问他所信的佛祖在哪之时，即使心有不甘，意气难平，玄奘仍会坚持双手合十，因为那是他的信仰，玄奘的信仰又岂会因苦难或是欲念改变呢？ 真正的信仰是不能祈求任何回报的，有了回报信仰就会变味为一种交易，这信仰简单的 说，大概就是一种希望吧。 无法确定最终的结果好坏，只是朝着看到的那个可能走去，这条道路就仿若没有边际的大海，隐隐约约有一个彼岸的轮廓，可并不知道它在哪里，信者只是因为信仰其存在而朝那里走去，而不能稍有变更。 倘使有丝毫预期回报的可能，这信仰难免沦为一种谋略，最终蜕变成不择手段的夺取，甚至是光荣，也可能腐蚀信仰。没有了世人的景仰，信仰还需坚持吗？以苦海里的挣扎作为回报的投资，抑或是以荣耀作为信仰的期许，那都不是佛祖对于玄奘的期盼，更不是玄奘自己的希望。","link":"/2021/02/09/%E8%A7%82%E5%90%8E%E6%84%9F/"},{"title":"认知、组织、决策和执行","text":"大约一年前，也就是2023年的8月，我即将读直博四年级，彼时由于政治经济等各方面因素叠加，中国和美国之间产生了各个方面的较量，对于我来说，最明显是在舆论宣传上。相比于我的少年时代，这些年国内对美国的宣传定位是从灯塔逐步到洼地，具体来说，是从21世纪初“臭公知”口中自由民主的典范到今天“爱国战马”口中煽风点火的邪恶化身。 此时我也即将结束学生生涯，一方面震撼于舆论宣传的变化，另一方面也需要对未来做出规划。然而当我将学校受到的“教育”和我能接触到的有限社会人士进行相互作用时，我更震撼于学校接受到的“伟大光荣正确价值观”并不是社会公民的共识，以及这种“伟大光荣正确价值观”甚至谈不上主流。生存以及如何生存得更好才是每一位社会公民最为关心的问题。 除此之外，一些更尖锐的问题下公民所表现出的行为倾向更让我感到“教育”和实践的违和，每当有新的这种事出现时，我都要看很久的人民日报和新闻联播才能稳定我的情绪和价值观。但是归功于传播技术的发展，这种事情发生的太多太频繁，以至于这种冲击严重占用我的大脑思考时间，影响了我的生活安排。因此我觉得有必要重塑价值观，使生活和目标可以正常进行。 当时我提笔写了第一篇有关个人认知的文章《置身事外：学校教育和社会政策间的鸿沟》，尝试从认知塑造这个角度去构建我对世界的理解。时隔一年，我又结合我在各种组织中习得的经验，完成各类目标后的成功或失败教训，对人在群体社会中的自我实现路径进行了总结。 为了完成人在群体社会中的自我实现，首先必须考虑认知，必须清楚人的认知来源于哪里；其次必须要分析组织，人生活在社会中，个体构成组织，组织的特点应当由个体的特点所推导。然后需要明白自我实现的过程，第一是决策，方向不能错；第二是执行，行者无疆。 我将首先给出《认知》《组织》《决策》《执行》指导日常生活行为的实践框架。此后我将补充对应的分析和案例，并可能适当修改框架的一部分内容。 认知123456789101112认知└── 来源 ├── 动物：身、心、脑 ├── 实践：效率低 但可验证 ├── 传播：不保真└── 属性 ├── 集体 ├── 时变└── 目的 ├── 认识世界 ├── 自我实现的起点 组织1234567891011121314151617181920组织└── 兴衰 ├── 形成：共同纲领 ├── 维系：产生增量 ├── 终结：分裂重组└── 结构 ├── 决策 ├── 管理 ├── 执行└── 范畴 ├── 个体 ├── 组织内 ├── 组织外└── 权力 ├── 保障实践的暴力权 ├── 控制传播的舆论权 | ├── 传播信息的权力 | ├── 控制传播渠道的权力 | ├── 拒收信息的权力 组织权力政治的本质是研究个体如何在组织中争夺权力。 组织的权力来源于对个体认知的塑造，宣传控制认知的传播来源，暴力保障认知的实践来源。 面对组织外部压力时，暴力权更容易形成。舆论权的目的在于塑造他人认知，基于逻辑和实践的舆论更有效果。当今世界美国尤其擅长舆论权的掌控，这可能和他们的建国历史有关，在18世纪末建国之前，各个殖民地孤悬海外，外部力量干涉效果弱，应对外部势力的持续暴力权较弱。相对来说，舆论权对于美国决策和执行的干涉效果更为明显。 组织的权力范畴既对外也对内，个体在组织中需要警醒并向这两种权力呈现出明面上的妥协。这两种权力是组织规则的根本来源。当个体在组织中的权力无法支撑其生态位时，他的地位就岌岌可危。 决策12345678910111213141516决策└── 为了自我实现的个体决策 ├── 决策基石 | ├── 面向过去：梳理总结 | ├── 面向现在：适配当前执行力 | ├── 面向未来：长期规划 ├── 决策水平 | ├── 降低决策开销 | ├── 提高决策持续时间└── 在组织中的个体决策 ├── 提升生态位 | ├── 掌握舆论权 | ├── 掌握暴力权 ├── 自我实现 | ├── 获取环境信息 | ├── 自我实现的部分目标 在组织中的个体决策个体在组织中使用信息传播权，目的是为什么？ 在组织内部范畴：对于组织，团结集体，塑造共同经历形成共有认知；对于个人，实行信息传播就是提升舆论权，从而提升个人在集体中的地位。 在组织外部范畴：首先是塑造个人形象，为新的组织参与构造条件；其次是通过组织的展示，提升个人在更大组织中的地位。 例：对于旅行或者生活中随手拍的照片、视频等多媒体信息，个体如何使用信息传播权？ 相比于文字，这类信息更加感性，蕴含的逻辑思考较弱。一般只对自己有纪念意义的信息不会对组织内部范畴或者外部范畴起作用，这类信息不宜公开发布。只有涉及到组织的，并且在组织中具有一定话语权，且在更大组织中有正面影响力的话题，才可以公开发表。 使用信息传播权时可能会带来信息拒收，信息拒收取决于一般人的身心脑水平。因此信息传播权的使用要注意频率。 执行12345678910111213141516171819202122执行└── 调整认知的动物来源 ├── 情绪控制（心：气） | ├── 第一层：思维和行为统一，情绪无法剥离 | ├── 第二层：情绪与行为剥离，思维中混杂情绪 | ├── 第三层：情绪与思维和行为剥离，情绪为目标服务 ├── 行为控制（身：精） | ├── 精力总量：取决于身体状态 | ├── 精力恢复：避免由于常做一件事带来的厌倦 | ├── 精力消耗：好的习惯降低身体的疲惫 ├── 思维控制（脑：神） | ├── 形成逻辑（实践和传播结合的后天认知起点） | | ├── 构造外存，辅助记忆 | | ├── 构造概念，串联细节 | | ├── 构造场景，加快验证 | ├── 其他任务（定位信息系统bug为例） | | ├── 调试工具 | | ├── 调用栈查看 | | ├── 合理注释└── 自我实现的意志力 ├── 目标：各级目标和实现时间节点的明确 ├── 路径：实现各级目标的方法和持续时间的明确 何为意志力情绪控制是对心灵的控制，是气；行为控制是对身体的控制，是精；思维控制是对大脑的控制，是神。意志是自我实现的动力，统帅精气神，意志坚定则精气神合一，大幅增强执行力。 当认知、自我实现的目标与路径、环境变化不确定性降低时，决策可以维持的时间增加，保证同样决策收益的决策开销可以降低，自我实现的意志力不消耗在决策上，大幅加成执行力。","link":"/2024/07/29/%E8%AE%A4%E7%9F%A5%E3%80%81%E7%BB%84%E7%BB%87%E3%80%81%E5%86%B3%E7%AD%96%E5%92%8C%E6%89%A7%E8%A1%8C/"},{"title":"认知的传播来源","text":"认知具有三个来源：动物来源、传播来源、实践来源；也具有两个性质：时变性和集体性。 推敲信息价值，稳慎调整认知在社会活动中，对社会运行模型洞彻更为深刻的认知可以做出更合适的决策。注意到认知的动物来源是先天赋予，实践来源则积累速度慢。因此在面对大多数决策时，尤其是没有先验实践的决策当中，认知的传播来源将起决定性作用。除此之外，在依赖前人著述的探索活动中，准确无误的传播来源也将起关键作用。 而认知的传播来源是具有各类传播渠道的媒体，因此，在接受信息时，有必要考虑：传播渠道的可信性，信息内部的逻辑性，信息与外部的相互作用。传播渠道的可信和信息内部逻辑是可以理解的，信息与外部的相互作用表现为两个方面： 信息对外界的作用：首先，如果信息是有立场的，那么来源渠道的立场是否和信息表达的立场有关，如果构成利益相关方，则存疑；其次，信息产生的知识是否对已形成的认知产生大量冲击，如果是，则需要谨慎调整新信息和已有认知不相容的部分。 外界对信息的作用：外界不同客体对信息的评价将作为新的信息，成为对信息本身评价的重要标志；有些时候，信息本身的细节不重要，客体对信息的作用反而重要。 例 社区fandom上关于dota2和hearthstone的wiki：真实有效量大。因为传播渠道无审核，来自社区共识；信息内部有分类体系、自洽，信息可证伪；大量玩家使用且收益。同类例子：liquipedia；反面例子：baidu wiki，审核存在，没有社区审核机制（仅有举报机制）；信息内部存在主观判断；百度本身和一些医疗机构存在利益关系，部分网民受害； “中国人智商高，身体弱”：不可信。因为传播渠道属于三人成虎，无来源；信息内部缺乏比较标准、比较对象空泛、结论过宽；部分人相信属于认知的集体性质（盲从），许多人质疑。 宗教：不可信。因为传播渠道属于上古流传；部分宗教体系明确，但不可证伪；传教士属于既得利益集团。","link":"/2023/11/06/%E8%AE%A4%E7%9F%A5%E7%9A%84%E4%BC%A0%E6%92%AD%E6%9D%A5%E6%BA%90/"},{"title":"认知的实践","text":"高效是实践的难点。 正文如《置身世外》一文所述，推动认知的迭代旨在确立行动指南。 行动完成的衡量标准应当以物质世界的改变程度来和预设目标的偏差来确定。 通常认知并不对物质世界直接产生影响，而行动完成的过程往往需要人和物质世界的相互作用才能得以实现，因此认知的迭代是完成目标过程中的“中间件”，而不是终点。 在《认知的迭代》中我们探讨了认知的来源性指和迭代方法，侧重点在于认知的形成和变化，是行动指南的确立，而行动的具体过程是本文要探讨的认知的实践。 实践一方面是认知来源的组成部分，另一方面是行动指南落地的最后一公里。区别于认知的传播来源，实践的反馈是直观可评估的，高效（快速，高质量）是实践中的难点。 高效实践：构造高效实践的关键在于始终保持全局视野，以一种构造性的态度审视整个行动过程。 概念驱动：点构成线构造性实践中最关键概念是过程性的构造，即过程中会涉及到很多子模块，而这些子模块功能分立依赖，模块自身的独特和模块相互依赖的关系决定了整个过程的创新特征，从而共同为一个全局目标进行服务。区别于一般性的实践，构造性实践强调个体对于子模块的创造和组织，这也是构造性实践和一般实践活动最大的不同。 正因为构造的过程涉及到很多个细碎的模块，当处于构造的开始阶段时，如果既要去想如何设计这些子模块，又要去思考这些子模块之间的关系，这样的思考困难且切换代价高昂。因此在构造性实践的初始阶段，应当先将这些子模块用一些辅助存储（哈希、外存）都列出来，然后再梳理其中的关系。 因为子模块没有被真正地实现，需要从性质、属性和作用的角度理解子模块，此时子模块没有被实现，因此要采用一些概念辅助理解子模块。所谓概念驱动，就是在构造性实践地初始阶段，也就是子模块的选取过程中，将子模块哈希到某些概念进行理解。比如一个简单构造性实践：写文章，其中起承转合就是对于文章的四个子模块的哈希式概念映射。所谓外存，就是有些时候涉及到的子模块非常多，远不是四个子模块就可以概括，这时候就需要用到笔记本、计算机等设备暂时存储一些概念，然后进行分析。 而梳理子模块的关系过程，就是子模块组合形成构造性实践蓝图的过程，谓之点构成线。这里的复杂度比列出子模块的过程要高出一个量级，因此速度也会变慢，投入精力也会变多。而构造性实践是实现目标的关键过程，构造性实践决定了目标完成的速度，因此在这个过程当中，需要比对子模块的预期组成逻辑，使之符合目标的完成需求。完成这样的工作很多时候不是简单和短暂的，可能需要局部长时间的思考投入，本文的《匹配构造性实践和认知的动物来源》一节将对局部长时间投入方法做一些初步探讨。 实证驱动：快速迭代当完成子模块关系梳理后，构造性实践在实现过程中的核心难点是如何实现即时验证。由于涉及到许多子模块之间的关联和环境特征，构造性实践中需要全局目标实现后才能进行所有片段组合后的整体性验证，这样就有两个问题：一是全部完成后的验证时间周期远超子模块的实现周期，无法实现子模块的即时验证；二是子模块的组合结构使得整体性验证不容易追溯各个子模块可能会出现的问题。 而部分模块分别验证的难度在于如何获取对应标签，这是因为当全局目标整体实现时标签反馈更容易获得，而只实现子目标时环境无法对部分内容进行有效反馈。 解决这个问题的方式有两种：一是改变子模块的构成模式可以使得标签易于获取，然而编写程序时标签容易获得，作业的标签容易获得，论文质量好坏的标签却难以获得，工厂的产品生产也需要大量投资才能见到效果，即部分标签的获取难易程度和环境特征紧耦合，构造性实践的场景下即时标签往往不具备；二是通过子模块的完成性来证实子模块逻辑功能（有点类似强化学习中的时序差分算法），如果子模块可以实现最终目标的完成（不论好坏），但是缺乏验证子模块完成质量的手段，可以在当前时刻降低质量评价的主观指标所占比重，用子模块在构造性实践中的完成能力作为即时标签，在下一节《逻辑驱动》中，我们将讨论如何根据目标的实现程度定义模块的完成性。 需要认识到，第一种方式思路在于改变子模块构成模式使其匹配环境，第二种方式思路在于通过逻辑完成实现局部验证。但是在接收到验证标签后，这两种方法可能会带来子模块实践方式的更改需求。这是因为构造性实践体现了全局目标的实现过程，发生了对已有物质世界的更改，是一种变化和创造。而在构造性实践依据已有认知搭建子模块和实现蓝图后，一是已有认知可能和已有物质世界有差距，验证标签出现后需要做修正；二是当物质世界发生更改后，已有认知也需要做适当的修正；三是构造性实践同样依赖认知的动物来源，由于身心脑力的疲惫可能会导致实践中精力消耗导致的偏差，这部分偏差需要验证修改。 因此实证在被即时反馈的同时，也需要迭代升级。如前所述，迭代升级依赖标签对天赋认知（精力消耗）的偏差和对后天认知的偏差修正。精力消耗引起的偏差修正是可以被即时更改，而后天认知偏差却取决于个体的认知水平。一般来说，后天认知偏差引起的构造性实践中子模块迭代可能会具有很长的时间周期，与此同时会带来认知的迭代升级。 总体而言，在构造性实践中，实证驱动子模块的快速实现，迭代是子模块具体的完成方式。为尽快获得子模块的实证标签，用完成能力取代局部主观质量评估。而对于子模块的质量把控，用迭代升级取代认知中对子模块实现的完美追求。 逻辑驱动：降低开销降低对子模块非逻辑性功能的要求，避免无用的评价指标。在构造性实践中，模块的完成性是子模块的第一评价指标。 其中，模块的完成性是指在模块构建完成后，能够确保目标实现过程——即构造性实践的流程——的顺畅简约进行。考虑到构造性实践本质上是目标实现的过程，因此，流程逻辑的顺畅性直接关联到目标中特定部分的成功完成，而逻辑的简约性则决定目标的完成代价。简而言之，用目标的部分完成作为模块完成的评价指标。 这里又涉及到面向时间尺度的目标分类和子目标的确立方法，在《学习理论》中我们初步探讨了目标之间的联系。在构造性实践中，目标的导向应当是端到端的，所有子目标应直接服务于全局目标。这是因为构造性实践往往是对中短期目标的实现，而在中短期目标的实现过程中，根据认知的时变性，短期内认知的大幅度调整从而导致目标修正的情况比较少。这种形势下，端到端的目标导向有助于避免规划对全局目标贡献甚微的子目标，确保每个子目标都对实现全局目标具有明确且重要的作用。 换言之，端到端目标导向可以确保所有子目标有效地支持全局目标的达成，从而当我们采用这种评价体系时，可以确保模块的完成性和全局目标的实现逻辑直接关联。 由于构造性实践中部分模块的完成对应着全局目标的部分逻辑实现，因此这种子模块的实现激励方式可以称为“逻辑驱动”，这样做的优势在于更多专注于全局目标实现的逻辑简约和流畅，而不是局部的所谓美观和优雅。避免局部的过度完善带来的身心脑力的额外开销，从而减少对全局目标逻辑实现的不必要身心脑和时间开销。 匹配构造性实践和认知的动物来源在《概念驱动》一节我们谈到构造性实践的难点在于梳理子模块的关系，而梳理子模块的关系的难度在需要局部长时间的思考投入。 局部长时间的思考投入难在两方面：1.思维需要聚焦在当前的子模块散列表中；2.长时间不停止地思考。 我们通过外部存储的方式将需要思考的模块以概念的形式进行记录，所以思维聚焦的难度是可以克服的。而对于长时间不受干扰的思考，一方面人的认知有集体属性，会受他人干扰，另一方面是认知动物来源的影响。 《集体属性》中强调了避免认知集体属性干扰的方法，因此关键是如何利用认知的动物来源来克服这个难度。 首先应当认识到长期思考一件事，也就是谓之专注的品质，并不是适用所有环境。这是因为除了构造性实践，还有一些情况需要大量处理外界即时信息，处理更多的环境即时变化，这些情况下并不是越专注越好，取决于使用环境。当然在构造性实践中，这是很重要的能力。 其次，认知天赋中存在的两个特征使得长时间不停止思考变得困难。一是生存天赋：人在物竞天择的恶劣环境中，趋向于敏锐接收外界消息来保持对外界环境的警惕从而做出即时反应；二是繁衍天赋：多巴胺驱动的行事方法，为了使得种群得以延续，繁衍阶段给人带来的快乐会改变个体的行动模式，近年来技术的研究使得这种生物学快乐的来源被人工制造出来，除了繁衍带来的多巴胺，各种类型的官能刺激（短视频、极限运动等）带来的即时多巴胺更加夸张，但是长期不停止思考带来的多巴胺是需要构造性实践完成之后才可以大量得到的，短期内繁衍天赋使得人趋向于选择官能刺激。 生存天赋使得大脑需要额外处理外界信息，多巴胺需求使得思考模式被身体本能接管。但长期不停的思考本身是需要大脑全程参与的，所以来自认知天赋的这两种特征在构造性实践中是要务必避免的。而天赋（认知的动物来源）只能用天赋抗衡，试图用大脑的意志抵抗天赋是消耗巨大且作用未知的（内耗）。 幸好认知的动物来源中还有另外一种特征：维持已有状态的惯性。即人在进化中，趋向于降低自己的能量消耗，维持已有状态和环境进行交互。这有两点启发：一是当进入长时间不停止思考的状态后，比较容易维持思考状态；二是减少日常对官能刺激的接触，这样大脑更容易控制身体，降低多巴胺阈值，从而更容易进入长时间不停止思考的状态。 从这两点出发，利用认知天赋中的惯性特征完成长时间不停止思考可以分两步：在预备阶段，形成降低多巴胺阈值的惯性，减少对即时官能刺激的接触，降低即时官能刺激的频度；在维持阶段，选择没有不可避免事件干扰的时间段，从而形成局部的惯性。 我们从认知天赋的惯性特征出发，说明在预备阶段和维持阶段如何避免生存天赋和多巴胺需求带来的对长时间不停止思考的干扰。但是这里的一个关键问题是如何从预备阶段转入维持阶段，即思维从其他方向逐渐转移到局部的子模块散列表上的过程如何不停止思考，也可以称为启动阶段的逐步聚焦问题。 在启动阶段，一方面会有前序时间其他思绪的干扰，另一方面会大量接收外界消息。思维不聚焦，而长时间不停止的思考的基础是思维聚焦。一个好的方法是从感知信息状态切换到输出信息状态，用已聚焦的信息形成输出，填充环境信息，再进行输入，从而避免其他思绪和外界消息干扰。进一步地，获得更多聚焦内的信息，然后重复这个过程，直到进入不停止思考的维持阶段。 总结（难点和方法分析链）：构造性实践—梳理子模块关系—局部长时间投入—思维聚焦/长时间不停止思考—认知的动物来源：趋向于多巴胺驱动（繁衍）和感知环境信息（生存）—惯性维持存在启动阶段的逐步聚焦问题—自我输出填充输入逐渐形成一个充满聚焦内容信息的环境—从而进入惯性阶段形成闭环。","link":"/2024/01/22/%E8%AE%A4%E7%9F%A5%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"title":"认知的迭代","text":"改善认知理论，提升行动效率 正文学习是个体不断调整和更新自己认知的过程，以实现在现实世界中的目标。所以学习是一个迭代的过程，认知的更新是结果，其价值体现在形成的观念有助于完成现实世界的目标。在这里“认知”不仅指信息上的认知，也指身体心里上的潜意识“动作”。 认知的来源有三种：动物来源、传播来源、实践来源。动物来源的认知是天赋，对于这部分认知，个体难以改变，只能加以利用。所以个体能调整的是传播来源和实践来源。 将传播来源和实践来源的认知与动物来源的固有认知相适应，将认知的目标与运用过程相适应，才能提升行动效率。 天赋：认知的动物来源身体、大脑、心理状态和水平是一个人认知能力的基石，比如记忆能力，天赋异禀之人就可以过目不忘、就可以有语言天赋，这就是认知的天赋水平，很难改变。但是可以改变的是身心（身体、大脑、心理）状态。比如良好的睡眠可以放松大脑、合理的坐姿可以帮助身体维持在一个放松的状态，从而提升专注时长。 心理的理想状态是心无杂念，在当前时间节点，专注局部目标。避免全局目标、中期目标、以及认知的集体属性的干扰。实现高效心理状态的方法一般有：降预期、降负担、降对比。第一条降低期望值是指短期的工作不会对全局和中期目标造成太大影响，因此不必在短期过程内重视甚至思考长远规划的内容；第二条降负担是不必指担心在当前时间段没有完成或者局部目标完成效果不好，因为局部目标的设定是根据过去认知设定，与客观世界有差距，这些情况是必然出现的；第三条降对比是指个体在完成局部目标的过程当中，集体中其他人的完成信息只能成为参考，绝不能因此就影响了局部目标的设定和完成进度（合理利用认知的集体属性）。 身体的理想状态是浑身放松，没有生病，且能随时集中力量的感觉。实现高效身体状态依赖客观环境和身体习惯：良好睡眠、饮食合理、住的冷暖适中、避免奔波劳顿、身体肌肉处于舒适状态。对衣食住行、以及本身的感知需要在闲暇时养成感知和改善的习惯，通过培养习惯、改善环境使得身体比较容易进入高效状态。 大脑的理想状态是思路清晰，在局部目标界定的范畴内处理和产生新的信息。大脑的高效是在范畴内快速处理和产生结构性信息。实现高效大脑状态的方法一般有：1.初始阶段明确范畴，过程中减少范畴外的不必要发散；2.明确习得知识的运用形态，以运用形态为局部目标的重要构成部分；3.通过合理的缓存机制明确信息结构，人脑记忆容量有限，将接收信息及其联系用纸笔等较方便工具记录进行缓存。 理论：认知的传播来源理论是逻辑的体现。目标确定后，理论先行，即使对于局部目标，也要有局部的先验理论来明确接受信息的概念、范畴、结构和数据分布。即使这样的理论可能不完整或者错误，但是它是可以被修正的，没有先验理论的大脑状态是混沌的。先验理论也需要根据实证来进行批判和修正，从而形成符合当前客观世界，有助于实现目标的认知。 由于理论一般以定义、概念、分类结构、分布映射的方式呈现，因此在信息传递效率上是高效的，但是这样高效的知识结构也意味着抽象的表达，很少有直观的表达可以简单地描述一个理论，除非是理论中的一小部分结论。所以如果对理论直接进行学习，得到的东西也很难想到如何直接应用在完成客观世界的目标中。 因此，在传播来源中寻找学习理论时，切勿贪多贪全。除了通过理论自身宣称的逻辑、理论的集体和使用量评估其真实性外，具有高质量实证标签、可高效验证是评估一个理论是否值得深入学习和应用的关键指标。 实证：认知的实践来源对于理论来说，如果没有实证，则只能称为先验理论。实证说明了理论的具体问题和在完成目标时理论的运用形式，是所谓的知识到真正能够被运用知识的关键步骤。 实证的结构以事件形式呈现，以不同数据类型作为反馈。实证反馈以人的直观感受呈现则信息密度很低，但是可以明确部分细节的高质量标签；以图的形式呈现则信息密度相对较高，可能存在信息缺失。代码体现了理论的实证结构，反映了运用形式，但是不可辨真伪。完整的实证应当包括事件结构和反馈。 由于客观世界的条件，实证很耗费时间和精力，实证效率无法跟上理论习得的速度，因此观念的迭代以及目标的完成很多时候瓶颈在实证。因此高效实证是有必要的，一般来说，高效实证取决于工具是否易得和熟练程度、取决于反馈的设计。 在工具的选取和使用过程中：首先要选择简单、易得、稳定、可复现的工具，如果工具依赖其他工具，除非无可替代，否则减少人工控制工具链的情况，尽量避免选取工具的互相依赖性；其次要界定工具的使用范畴，工具是不能专门学的，只有根据需求确定工具使用流程，减少范畴外的发散使用；如果是长期使用的工具，尽量体会工具的设计观念，对工具有理论认知，可以更高效理解工具的不同用法；最后避免纠结工具的不完美，工具为了适应客观世界的具体场景，是有很多特殊设计，这种设计导致了工具的设计不整齐，甚至存在bug，但是只要不妨碍理论实证过程，这些问题需要被忽视。 工具辅助实证结构的完成，但并不是完整的实证，除了直观的反馈以外，基于理论认知设计较高层次的反馈有助于高效完成实证，比如反应数据分布、反应数据类型、反应数据结构的各类图表，可以帮助对理论体系的结论进行直接快速验证，这样的设计也可以实现对多个数据点的验证，避免重复的直观实证。如果数据量较少，可能需要进行数据增强，基于已有数据构造，这一般是针对工具较难获得的情景。 理论和实证的关系在一个局部目标当中，理论和实证是相互依赖，螺旋上升的关系。 以下呈现了两种认知来源在学习过程中性质、目标的对比。 认知来源体现 相似特征 区别特征 追求目标 传播来源：理论 不可或缺 抽象性、信息高效 易于实证 实践来源：实证 不可或缺 直观性、可评估性 验证高效 局部-中期-模块-全局目标的联系从时间尺度来说，全局目标的时间尺度横跨数年甚至十数年，包含大量模块，中期目标的时间尺度以季度或者年为单位，局部目标一天内可以有两三个。 全局目标包含方向不同的模块，需要模块间相互配合。比如全局目标可以是取得多少的金钱收益，这个目标缺少具体的实现路径，可以划分为子模块，比如初始的金钱收益从哪里获取，多少收益需要从投资收益获得，如果打工能否实现这些收益等等，从而将全局目标转化为找到什么样的工作，从事什么行业，需要人与人之间的交流技能更多还是专业技能更多等等。全局目标划分为模块后，根据模块的具体目标分配时间和精力资源，在一定时间内以阶段式的方法实现这些模块，比如根据全局目标确定从事人形机器人行业，需要做到行业的专家级别。这就是全局目标里的一个模块。 全局目标的一个模块的实现也不是一蹴而就的，可能也需要数年时间才能完成，一个模块的完成由若干中期目标组成。比如做到行业的专家级别，需要取得博士学位，在过程中深入研究，那么完成研究工作就是其中一部分，完成研究论文就是具体体现，这里完成研究论文就是由全局目标拆解得到的中期目标。注意完成研究论文可能有助于取得博士学位，但可能对成为行业认可的专家帮助不大，因此这里可能有中期目标和全局目标设置的权衡。 中期目标的完成也不是立竿见影的，中期目标由每天若干局部目标组成，局部目标旨在快速得到反馈，以确认整体先验理论对于中期目标的设定是合理的，通过运用形式的建立和持续反馈培养良好的心理状态和调整中期目标。比如发一篇CVPR，可能是只需要一个月的事情，如果之前预计消耗时间是三个月，就需要调整中期目标，为中期目标和全局目标权衡提供时间尺度的保障。 局部-中期-模块-全局目标的大方向是一致的，由于认知的集体属性，在目标的完成路径上，会有局部小方向的不一致性。一般来说，局部目标是管控的，中期目标是相对可预期的，这并不意味着好预期，只是相对全局目标和其模块来说。而局部目标是又可以得到即时反馈，在这种反馈过程中，由于完成路径的范畴相对集中，中期目标得到的经验相对更多，因此中期目标会进行更多的时间尺度调整。 全局目标-模块向中期目标的拆解是一个全局问题，局部目标的反馈会偶尔得到相关经验，因此尽量避免反馈触发式地大量思考全局目标设定，根据中期目标完成情况周期性更新全局目标-模块的设定，这个周期应对标中期目标的时间尺度，2-3个月重新规划一次比较好。 关键词两个“适应”，身心脑状态、理论可验证、高效实践、局部-中期-全局","link":"/2023/11/16/%E8%AE%A4%E7%9F%A5%E8%BF%AD%E4%BB%A3/"},{"title":"认知的传播来源和集体属性","text":"合理利用人类认知的集体属性。 动机：认知的集体属性“认知的迭代”一文中，在对认知的动物来源分析时，提到过如何维护身心脑的状态保持在一个较高的水平上。具体来说，关于心理状态要实现三个“降低”：降负担、降预期、降对比。负担、预期、对比一方面来自认知在实践中的时变性，另一方面也来自群体交往中认知的集体属性作用。 集体属性一方面可以提升认知的边界，但是另一方面，集体属性会对已建立认知形成冲击。总的来说，在认知迭代方面，集体属性是弊大于利的，因为集体属性对于认知施加的变化不是实践渠道，而是传播渠道：一方面理论内容较少；一方面实例缺乏验证，依赖传播渠道的准确性。当然，某些情况，比如在某一方面没有先验理论的情况，或者先验理论弱于集体属性中传播的认知时，可以适当利用认知的集体属性建立先验理论。 换言之，认知的集体属性不应该在认知的学习理论中发挥主要作用。 方式：异步交流那么认知的集体属性应该如何发挥作用呢？认知的集体属性应该在实践中发挥作用，一个具有一定认知能力的个体，无论是动物或者人，当他们和自己不建立情感这种同步联系时，他们体现价值的地方在于实践中的劳动（任务卸载）。 任务卸载是分布式系统中的经典问题，如果我们把认知的集体属性应用在实践的劳动上，很多模型可以套在这种协作模式上。异步是任务卸载的核心机制。反映在人与人的劳动协作模式上，就是交流的非即时响应。信息的交流是有代价的，就像网络中长连接的建立（TCP），如果试图利用这种集体属性（信息共享）迭代认知，这种过程就像数据挖掘，消耗更大资源，而且数据质量严重影响数据挖掘效果【从这个角度讲，利用认知集体属性迭代认知这种方式，大部分情况也是完全没有必要的】。 因此对于低优先级任务、服务质量要求低的任务使用非长连接（UDP），非即时响应的方式。只对少部分服务质量要求高，更有收益的任务采用长连接的方法。 限制：规则和信息价值当然，并不是说要一成不变在基于信息交流的实践中采用这种方法。异步的方式需要考虑约定规则的容忍度，在无法打破约定规则的情况下，应按照约定的方式进行沟通。 这是因为：1.信息本身尽管数据偏好严重但暴露了参与交流方的特点；另外有些信息价值高，可以在某一方面辅助建立先验理论。【但是对于交流信息的数据挖掘大部分情况是异步、且优先级极低的。需要沉淀一定的规模，才能进行。】2.打破规则的代价更大，带来的实践负收益超出异步交流的正收益。","link":"/2023/12/20/%E9%9B%86%E4%BD%93%E5%B1%9E%E6%80%A7/"},{"title":"归档","text":"一些思考后给出答案，但是不成体系的问题。 概率空间定义要用三个要素概率空间定义，样本空间、事件空间、概率测度 我们直觉上、习惯上是将概率定义在样本空间上，然而实际上严格定义是在事件空间上，事件空间是sigma代数， 这是为了解释当样本空间有不可数集的时候，如何定义概率的问题。 泛函“只”有一阶导泛函是函数向实数域的映射。 泛函取极值的必要条件是euler-lagrange方程成立。 我们看到的euler-lagrange方程是依赖于路径、路径导数和自变量的微分方程。 广义的euler-lagrange方程依赖路径的更高阶导数。 最速降线没有用广义euler-lagrange方程只是采用euler-lagrange方程的标准形就能解出来路径方程，并且和事实相符。 动态规划是强化学习动态规划是有模型强化学习。有模型强化学习不需要通过数据进行训练。动态规划的不同状态是指具有不一样的参数的同一个优化问题。 比如背包问题，背包容量不一样，物品数量不一样，就是一个状态。 形式化成一个优化问题后，发现可以通过max，min或者分段函数这样的非线性操作进行状态转化，转移函数就对应强化学习里的模型。 动态规划能解的问题很有限，不能有后效性，且初始子问题足够简单，才能求解。找转移函数的过程比较困难，需要积累。 背包dp，序列dp，树状dp，DAG-dp等。 Caffe代码结构caffe组织推理的过程分为四层结构：syncedmem -&gt; blob -&gt; layer -&gt; net syncedmem是最底层的线性数据结构，用来维护cpu和gpu的存储同步。blob是更高一层的抽象，维护张量的形状和数据，是所有张量操作的基本类型。layer是构成神经网络的基本单元，net就是最终的推理结构。 Caffe除了完成推理，还要执行训练，对异构设备的适应，以及对单独CPU执行的支持，因此Caffe对于内存进行syncedmem这一层的封装。我们只需要在前向推理时实现多流并发，syncedmem这一层封装可以不需要，可以在blob内部完成内存和显存的数据转化和分配。 im2col对卷积的实现二维卷积的朴素实现包括7个循环：batch_size，输出通道数量、输入通道数量、卷积核高、卷积核宽、沿宽方向滑动距离、延高方向滑动距离。以此类推，三维卷积9个循环。 这么多循环放在CUDA中，不利于优化，且缓存不命中，频繁访存执行效率低。考虑到卷积核和输入张量对应点的相乘，与向量点乘的运算过程是一样的，而矩阵乘法是向量点乘的堆叠，因此可以设计一种先将输入转化成矩阵，然后执行矩阵的相乘加速卷积速度的方法。 方法改变，结果不改变，应保证转化+矩阵相乘与朴素实现的输出结果一样。 朴素卷积输出张量$O$的维度是 [batch_size, conv_out_channels, spatial_output_shape_h, spatial_output_shape_w]， 卷积层保存的权重$W$维度是 [conv_out_channels, conv_in_channels, kernel_h, kernel_w]。 卷积核的维度就是 [conv_in_channels, kernel_h, kernel_w] 设$O = W * D$，$D$是输入张量转化而来形成的矩阵。考虑到张量在内存和显存中均是一维存储，可以将$W$视作行数为conv_out_channels，列数为conv_in_channels * kernel_h * kernel_w的矩阵，$O$视作行数为batch_size * conv_out_channels，列数为spatial_h * spatial_w的矩阵。 转化得来的矩阵$D$的行数需要是$W$的列数，列数是$O$的列数，才能满足$O = W * D$的表达式。同时左矩阵的行向量与右矩阵的列向量点乘需要等价于卷积核和张量对应区域的相乘。 考虑batch_size中的输入张量独立，因此可将$O$划分成batch_size个张量，$D$的行数为conv_in_channels * kernel_h * kernel_w，列数为spatial_h * spatial_w，从而首先确定维度。其次，根据$O$中列号对应的输出张量空间位置，计算在原始输入张量的空间位置中的哪些元素与卷积核相乘，在输入张量中按行展开这些元素，按卷积核通道序号次序放入列号对应的列向量。 计算方式如下代码所示。 1234567891011121314151617181920212223/* compute the output_spatial_shape */int height_col = (height + 2 * pad_h - (dilation_h * (kernel_h - 1) + 1)) / stride_h + 1;int width_col = (width + 2 * pad_w - (dilation_w * (kernel_w - 1) + 1)) / stride_w + 1;CUDA_KERNEL_LOOP(index, conv_in_channels * height_col * width_col) { /* compute the first position in input spatial tensor */ const int c_im = h_index / height_col / width_col; // the number of conv_in_channels const int h_offset = h_col * stride_h - pad_h; const int w_offset = w_col * stride_w - pad_w; const float* data_im_ptr = data_im; data_im_ptr += (c_im * height + h_offset) * width + w_offset; // the first position /* the corresponding position */ for (int i = 0; i &lt; kernel_h; i++) { for (int j = 0; j &lt; kernel_w; j++) { int h_im = h_offset + i * dilation_h; int w_im = w_offset + j * dilation_w; *data_col_ptr = (h_im &gt;= 0 &amp;&amp; w_im &gt;= 0 &amp;&amp; h_im &lt; height &amp;&amp; w_im &lt; width) ? data_im_ptr[i * dilation_h * width + j * dilation_w] : 0; data_col_ptr += height_col * width_col; // next row } }} 从输出张量反推输入张量位置是计算的关键步骤，对应即是h_offset、w_offset以及data_im_ptr的计算过程。卷积的整体前向传播过程即分为两步。 123456789101112131415161718192021222324252627void ConvolutionLayer::Forward(const std::vector&lt;Blob*&gt;&amp; bottom, const std::vector&lt;Blob*&gt;&amp; top, cudaStream_t stream) { const float* weight = this-&gt;blobs_[0]-&gt;gpu_data(); for (int i = 0; i &lt; bottom.size(); i++) { const float* bottom_data = bottom[i]-&gt;gpu_data(); float* top_data = top[i]-&gt;mutable_gpu_data(); for (int n = 0; n &lt; num_; n++) { // bottom_data -&gt; col_buffer_ // weight * col_buffer_ = top_data im2col(bottom_data + n * bottom[0]-&gt;count(channels_), channels_, bottom[0]-&gt;shape(channel_axis_ + 1), bottom[0]-&gt;shape(channel_axis_ + 2), kernel_h_, kernel_w_, pad_h_, pad_w_, stride_h_, stride_w_, dilation_h_, dilation_w_, col_buffer_-&gt;mutable_gpu_data(), stream ); acc_gemm(CUBLAS_OP_N, CUBLAS_OP_N, conv_out_channels_, conv_out_spatial_count_, kernel_count_, 1.0, weight, col_buffer_-&gt;mutable_gpu_data(), 0.0, top_data, stream ); if (bias_term_) { acc_gemm(CUBLAS_OP_N, CUBLAS_OP_N, conv_out_channels_, conv_out_spatial_count_, 1, 1.0, this-&gt;blobs_[1]-&gt;gpu_data(), bias_multiplier_-&gt;gpu_data(), 1.0, top_data + n * conv_out_channels_ * kernel_count_, stream ); } } }} C语言宏的替换第一种，被替换对象是被标点或空格分割开的“独立字符串”，用#表示。第二种，被替换对象被一个独立字符串真包含，用##表示，当被替换对象位于独立字符串开头，则在被替换对象的结尾加##表征；当被替换对象位于独立字符串结尾，则在被替换对象的头部加##表征；当被替换对象位于独立字符串中间，则在被替换对象的开头和结尾都加##表征。 例子： 12#define REGISTER_LAYER_CREATOR(type, creator) \\ LayerRegisterer g_creator##type(#type, creator); \\ C语言静态库静态库方便移植，速度快，但是体积大，编译慢，这个都知道。除此之外，静态库中的static变量不会被事先声明，如果在头文件定义static变量，可能有重复定义的风险。 怎么解决呢？制作一个只有可能被应用程序引用的头文件，在其中定义需要预先定义的static变量。 例子：不同layer初始化注册map表。 12345678910111213/* register_layer.h */#ifndef REGISTER_LAYER_H#define REGISTER_LAYER_H#include &quot;u1/layer_factory.h&quot;#include &quot;u1/layer/full_layer.h&quot;// ...REGISTER_LAYER_CLASS(Full);// ...#endif C++内存分配 C++为了避免手动free或者delete的问题引入std::shared_ptr&lt;&gt;，即智能指针。智能指针调用对象方法时和一般指针一样，变成一般指针时用.get()方法. 当具有独显时： C++仅有分配内存的能力，显存的分配用CUDA接口； gpu计算单元不能访问内存，cpu也不能访问显存， 显存和内存可以互通，但是显存分配不提供智能指针，仅提供原始类型的指针。 core dump一般逻辑问题查找日志基本可以解决。Core Dump没有日志报错信息，或者日志容易出问题。这时需要用调试工具。 使用gdb+cmake编译时，添加-g编译选项，如下段代码。用ulimit -c nolimited取消core dump文件的大小限制。更改/proc/sys/kernel/core_pattern的core dump文件输出位置和命名，然后用gdb &lt;程序名&gt; &lt;修改后的coredump文件名&gt; 进入gdb调试。在gdb中用bt查找调用栈，一般能定位内存出问题的位置。 1234# CMakeLists.txtADD_DEFINITIONS(-g -D_REENTRANT -D_FILE_OFFSET_BITS=64 -DAC_HAS_INFO -DAC_HAS_WARNING -DAC_HAS_ERROR -DAC_HAS_CRITICAL -DTIXML_USE_STL -DAC_HAS_DEBUG -DLINUX_DAEMON) 工厂模式的好处神经网络前向推理初始化的时候，每个层需要根据InferParameter中定义的layer编号类型定义并创建。不同类型的layer对应抽象layer的不同子类实现，如果没有工厂模式的话，需要在初始化layer的时候，根据layer的类型一一对应写出其构造函数，当类型很多时，这一步很繁琐。 用工厂模式，就是初始化的时候很简单，一行代码，工厂模式自己找到对应的构造函数，如下所示。 12345for (int layer_id = 0; layer_id &lt; para.layer_size(); layer_id++) { // ... layers_.push_back(LayerRegistry::CreateLayer(para.layer(layer_id))); // ...} 这个函数对应实现如下，在一个全局维护的map中，根据layer类型查找对应的初始化函数。 1234567891011121314151617181920212223242526#ifndef LAYER_FACTORY_H#define LAYER_FACTORY_H#include &quot;u1/layer.h&quot;#include &quot;u1/proto/test.pb.h&quot;class LayerRegistry { private: LayerRegistry() { } public: typedef std::shared_ptr&lt;Layer&gt; (*Creator) (const u1::LayerParameter&amp;); typedef std::map&lt;std::string, Creator&gt; CreatorRegistry; // create CreatorRegistry ... static std::shared_ptr&lt;Layer&gt; CreateLayer(const u1::LayerParameter&amp; para) { const std::string&amp; type = para.type(); CreatorRegistry&amp; registry = Registry(); if (registry.count(type) != 1) { std::cout &lt;&lt; &quot;Unknown layer type: &quot; &lt;&lt; type &lt;&lt; &quot;(known types: &quot; &lt;&lt; LayerTypeListString() &lt;&lt; &quot;)&quot;; } return registry[type](para); }};#endif 除此之外，工厂模式中每个layer都有自己的特殊SetUp函数，如下面第5行代码所示，也不用根据type类型再写额外的判断逻辑。（也是C++重载的好处？指向基类的指针调用不同子类的重载函数？）总之，工厂模式，就是为了在使用“工厂”的产品时，不用加一堆分支判断逻辑。（其实就两字：方便！） 1234567for (int layer_id = 0; layer_id &lt; para.layer_size(); layer_id++) { // ... layers_.push_back(LayerRegistry::CreateLayer(para.layer(layer_id))); // ... layers_[layer_id]-&gt;SetUp(bottom_vecs_[layer_id], top_vecs_[layer_id]); // ...} 那么工厂模式如何实现的呢？第一，多种layer实现时需要采用继承的方法；第二是维护一个全局的初始化函数表，这样在初始化时查找这个表就可以避免写判断逻辑，这个初始化表可以在类实现中直接注册。 123456789#define REGISTER_LAYER_CREATOR(type, creator) \\ LayerRegisterer g_creator##type(#type, creator); \\#define REGISTER_LAYER_CLASS(type) \\ static std::shared_ptr&lt;Layer&gt; Creator_##type##Layer(const u1::LayerParameter&amp; para) { \\ return std::shared_ptr&lt;Layer&gt;(new type##Layer(para)); \\ } \\ REGISTER_LAYER_CREATOR(type, Creator_##type##Layer) \\#endif 比如这段代码中，LayerRegisterer的初始化调用了一个static方法实现creator的注册，这两个宏实现了这个类的初始化，所以我们看到只需要一行代码就可以实现不同layer子类的注册。 CUBLAS加速矩阵乘法的一个小问题矩阵、张量在内存和显存中均以一维方式存储。 在CUBLAS中，GPU的计算单元读取矩阵的方式是按列读取和放回显存。我们习惯内存、显存以及cpu以行优先的方式读取。CUBLAS提供的API就不会很习惯，因此做一个封装使得外界使用时仍然按照习惯写法，但是内部仍用CUBLAS加速。 采用的方式调用CUBLAS的API的时候将A和B顺序反过来。这是因为CUBLAS读取的是转置，若$C=AB$，则$C^T=B^T \\times A^T$，而$C^T$在GPU计算单元写回的时候是列优先顺序，则按照行优先读取的时候就是$C$，所以在封装内部调用CUBLAS的API的时候将A和B顺序反过来即可。 代码如下 12345678910111213void acc_gemm( const cublasOperation_t cuTransA, const cublasOperation_t cuTransB, const int M, const int N, const int K, const float alpha, const float* A, const float* B, const float beta, float* C) { int ldb = (cuTransB == CUBLAS_OP_N) ? N : K; int lda = (cuTransA == CUBLAS_OP_N) ? K : M; cublasHandle_t cublas_handle_; CHECK_CUBLAS(cublasCreate(&amp;cublas_handle_)); CHECK_CUBLAS(cublasSgemm(cublas_handle_, cuTransB, cuTransA, N, M, K, &amp;alpha, B, ldb, A, lda, &amp;beta, C, N)); CHECK_CUBLAS(cublasDestroy(cublas_handle_));} caffe中blob的初始化方式caffe，又称blobflow。 caffe是用来实现各种各样神经网络前向推理，和反向传播的框架。神经网络由各种layer组合构成，caffe自身提供了卷积、全连接等层的实现。 由于layer的是存储密集、计算密集型的任务，且适用并行加速，因此涉及到cpu、内存、显存和gpu之间的切换。另外layer的输入、输出，自身权重都是高维张量构成，而数据在内存和显存的存储为一维结构，因此维度、数据的长度、显存和内存的同步这三个因素需要被考虑。 caffe用blob表征权重、输入和输出等张量。blob逻辑上描述了张量维度、展平后的数据长度。也描述了blob数据对应的导数数据，如果只做前向传播的话这部分数据是用不到的。blob的默认初始化不分配内存和显存，张量维度和展平后数据长度默认为0。blob的显示初始化对应分别对应权重张量和输入输出张量的存储分配。 权重的张量维度、展平后的数据长度在模型文件中被定义，因此caffe定义第一种初始化和保存方式： 12void FromProto(const BlobProto&amp; proto, bool reshape = true);void ToProto(BlobProto* proto, bool write_diff = false) const; 针对前向推理以及反向传播的中间计算结果、即输入张量和输出张量，caffe定义了第二种初始化方式：这种初始化方式调用了Reshape函数，Reshape当中会为数据分配内存和显存。 123456789explicit Blob(const vector&lt;int&gt;&amp; shape);template &lt;typename Dtype&gt;Blob&lt;Dtype&gt;::Blob(const int num, const int channels, const int height, const int width) // capacity_ must be initialized before calling Reshape : capacity_(0) { Reshape(num, channels, height, width);} 和第一种方式的区别是这种方式只定义张量维度，不对数据的值做要求，采用这种接口的原因是中间张量的数据并不是我们需要保存的权重，只是需要在显存中开辟张量存储的空间。 由于前向推理时大部分Layer的设计中，（1）输出张量的维度依赖输入张量的维度，（2）并且上一层的输出张量一般是下一层的输出张量，因此Layer在初始化时，一般要求输入张量bottom的维度确定并完成内存显存分配，在调用SetUp函数时再确定输出张量top的维度并完成存储空间分配。多个Layer构成的推理流在初始化时，也要遵循这一设定。 123456void SetUp(const std::vector&lt;Blob*&gt;&amp; bottom, const std::vector&lt;Blob*&gt;&amp; top) { CheckBlobCounts(bottom, top);// bottom blobs need to be allocated shape and data memory before, where top blobs don't. LayerSetUp(bottom, top); Reshape(bottom, top);} CMake编译系统环境gcc7.5.0，ubuntu18.04，cmake3.25.0。 整体结构如上图所示，u1文件夹里是写好的算法代码，eval文件夹里是基于算法编写的应用程序，包括测试、实验对比、调用程序等。因为用到protobuf，所以需要先对proto文件处理生成对应*.pb.cc和*.pb.h文件，这两个文件内容比较多，先把其编译成独立的库，而不是直接和u1里的代码混编，第二步编译u1，最后编译eval里的应用程序。 有几个点注意：第一头文件和执行文件放在一起，所以不设置单独include文件夹，第二cmake版本小于3.10需要手动引入cuda库。 因此CMakeLists.txt的书写方法如下： 123cmake_minimum_required(VERSION 3.10)project(u1_test CXX C CUDA) 先声明项目需要用到语言，因为是GCC7.5.0，默认C++11标准。 123include_directories(&quot;${PROJECT_SOURCE_DIR}/src&quot;)aux_source_directory(&quot;${PROJECT_SOURCE_DIR}/src/u1&quot; u1) 声明头文件额外引入路径和需要编译的库目录 123find_package(gflags REQUIRED)find_package(glog REQUIRED)find_package(protobuf REQUIRED) 声明需要用到的包，glog，gflags和protobuf是我做C++开发比较喜欢用的库。 12345678910111213141516# protoc -I=. --cpp_out=. kernel.protoset(proto_file_name &quot;kernel&quot;)get_filename_component(u1_proto &quot;${PROJECT_SOURCE_DIR}/src/u1/proto/${proto_file_name}.proto&quot; ABSOLUTE)get_filename_component(u1_proto_path &quot;${u1_proto}&quot; PATH)set(PROTO_GENERATE_DIR &quot;${PROJECT_SOURCE_DIR}/src/u1/proto&quot; )set(u1_proto_srcs &quot;${PROTO_GENERATE_DIR}/${proto_file_name}.pb.cc&quot;)set(u1_proto_hdrs &quot;${PROTO_GENERATE_DIR}/${proto_file_name}.pb.h&quot;)add_custom_command( OUTPUT &quot;${u1_proto_srcs}&quot; &quot;${u1_proto_hdrs}&quot; COMMAND protoc ARGS --cpp_out &quot;${PROTO_GENERATE_DIR}&quot; -I &quot;${u1_proto_path}&quot; &quot;${u1_proto}&quot; DEPENDS &quot;${u1_proto}&quot;) 生成kernel.proto对应的c++ API文件 123456789101112131415add_library( proto_ttt ${u1_proto_srcs} ${u1_proto_hdrs})target_link_libraries( proto_ttt protobuf::libprotobuf)add_library( u1_lib ${u1}) 然后分别生成库文件，包括定义接口的库以及算法库。 12345678910111213141516function(add_executable_app app_name app_path) add_executable( ${app_name} ${app_path} ) target_link_libraries( ${app_name} u1_lib gflags glog protobuf::libprotobuf )endfunction ()add_executable_app(test &quot;${PROJECT_SOURCE_DIR}/src/eval/test.cu&quot;)add_executable_app(main &quot;${PROJECT_SOURCE_DIR}/src/eval/main.cu&quot;) 最后生成应用程序、测试程序和用来比较的实验数据。 梯度下降现实世界中的诸多任务可以被抽象成一个接收一定形式的输入，并产生某种类型输出的函数。然而对于现实世界中的大部分任务，其函数关系复杂且普遍具有非线性关系，无法轻松找到这样的函数。 神经网络算法的目的就是通过神经网络结构表征任务的非线性特征，并通过已有数据集产生神经网络参数，从而拟合现实世界任务对应的函数，这个函数可以被表示为 其中$structure$表示神经网络的结构，该结构是神经网络拟合现实任务非线性特征的关键，通常情况下是先验的，需要研究人员精心设计得到，比如transformer和conv结构。 $X,Y$表示数据集中的输入和输出，参数$w$是神经网络参数。在模型获取阶段（训练），我们能够同时拿到数据集的输入和输出$X,Y$，并希望得到神经网络参数$w$；在模型应用阶段（推理），我们只拿到数据集中的$X$（输入数据），希望得到和真实世界任务尽可能相近的结果$Y$。 在模型推理阶段中，由于我们已经得到了神经网络参数$w$，获得结果的过程是一个前向计算的过程，按步骤执行即可。 而在模型训练阶段，神经网络参数$w$是未知的，而函数$O(X)$的输出是已知的，由于神经网络参数$w$通常是高维向量，数据集$X,Y$同样具有一定规模，这相当于是在近似求解一个高维方程，这个问题通常是难解的，因此需要梯度下降，也就是反向传播过程。 将近似求解高维方程的问题转化为最小化loss函数的问题，令 在反向传播过程当中，由于数据集中的输入和输出都是已知的，即让该函数最小即可，根据优化理论，按照下式不断迭代，直到loss函数的变化小于某阈值即可。 这里讨论一些梯度下降方法中确保准确率的一些难点。 关于数据集：由于数据集是通过实践获得的，只有统计意义上的理论保证。所以在每一轮迭代过程中如何选取数据集，如何保证数据集之间的偏差在一定范围内？ 关于神经网络结构：由于神经网络的层状结构，存在梯度消失，梯度精度等问题 Lipschitz Continuity In mathematical analysis, Lipschitz continuity, named after German mathematician Rudolf Lipschitz, is a strong form of uniform continuity for functions. Intuitively, a Lipschitz continuous function is limited in how fast it can change: there exists a real number such that, for every pair of points on the graph of this function, the absolute value of the slope of the line connecting them is not greater than this real number; the smallest such bound is called the Lipschitz constant of the function (or modulus of uniform continuity). For instance, every function that has bounded first derivatives is Lipschitz continuous. We have following chain of strict inclusions for functionsover a closed and bounded non-trivial interval of the real line (So strong!) Continuously Differentiable $\\subset$ Lipschitz Continuous$\\subset$ $\\alpha-$ Holder Continuous $\\subset$ Uniformly Continuous = Continuous Lipschitz Continuity Defintion: Given two metric spaces $(X, dX)$ and $(Y, dY)$, where $dX$ denotes the metric on the set $X$ and $dY$ is the metric on set $Y$, a function $f : X \\rightarrow Y$ is called Lipschitz continuous if there exists a real constant $K \\geqslant 0$ such that, for all $x_1$ and $x_2$ in $X$, dY(f(x_1), f(x_2)) \\leqslant KdX(x_1, x_2)Metric SpaceIn mathematics, a metric space is a nonempty set together with a metric on the set. The metric is a function that defines a concept of distance between any two members of the set, which are usually called points. The metric satisfies a few simple properties: the distance from $A$ to $B$ is zero if and only if A and B are the same point. the distance between two points are positive. the distance from $A$ to $B$ is the same as the distance from $B$ to $A$, and the distance from $A$ to $B$ is less than or equal to the distance from A to B via any third point C. Covering Number and Packing NumberIn mathematics, a covering number is the number of spherical balls of a given size needed to completely cover a given space, with possible overlaps. Two related concepts are the packing number, the number of disjoint balls that fit in a space, and the metric entropy, the number of points that fit in a space when constrained to lie at some fixed minimum distance apart. Let $(Z, d)$ be a metric space, let $K$ be a subset of $Z$, and let r be a positive real number. Let $B_r(x)$ denote the ball of radius $r$ centered at $x$. A subset $C$ of $Z$ is an $r-external$ covering of $K$ if: K \\subseteq \\cup _{x\\in C} B_r(x)In other words, for every $y \\in K$ there exists $x \\in C$ such that $d(x,y) \\leqslant r$. If furthermore $C$ is a subset of $K$, then it is an r-internal covering. The external covering number of $K$, denoted $N_{r}^{\\text{ext}}(K)$, is the minimum cardinality of any external covering of K. The internal covering number, denoted $N_{r}^{\\text{int}}(K)$, is the minimum cardinality of any internal covering. A subset $P$ of $K$ is a packing if $P \\subseteq K$ and the set ${B_{r}(x)}_{x\\in P}$ is pairwise disjoint. The packing number of $K$, denoted $N_{r}^{\\text{pack}}(K)$, is the maximum cardinality of any packing of $K$. A subset $S$ of $K$ is r-separated if each pair of points $x$ and $y$ in $S$ satisfies $d(x, y) \\geqslant r$. The metric entropy of $K$, denoted $N_{r}^{\\text{met}}(K)$, is the maximum cardinality of any r-separated subset of $K$. Covering number and packing number will be written as $N(Z, d, r), M(Z, d, r), $ respectively. Metric entropy also equals to $log N(Z, d, r)$.","link":"/2022/02/03/%E5%BD%92%E6%A1%A3/"}],"tags":[{"name":"随笔","slug":"随笔","link":"/tags/%E9%9A%8F%E7%AC%94/"},{"name":"笔记","slug":"笔记","link":"/tags/%E7%AC%94%E8%AE%B0/"},{"name":"总结","slug":"总结","link":"/tags/%E6%80%BB%E7%BB%93/"},{"name":"瞎扯淡","slug":"瞎扯淡","link":"/tags/%E7%9E%8E%E6%89%AF%E6%B7%A1/"}],"categories":[]}