---
title: Gradient Descent
date: 2021-11-08 09:56:17
tags: 算法
---

梯度下降如何应用在神经网络训练中?

<!-- more -->

## 背景

现实世界中的**诸多任务可以被抽象**成一个接收一定形式的输入，并产生某种类型输出的**函数**。然而对于现实世界中的大部分任务，其函数关系复杂且普遍具有**非线性关系**，无法轻松找到这样的函数。

神经网络算法的目的就是通过神经网络结构表征任务的非线性特征，并通过已有数据集产生神经网络参数，从而**拟合**现实世界任务对应的函数，这个函数可以被表示为

![](/img/Gradient-Descent/func.png)

其中$structure$表示神经网络的结构，该结构是神经网络拟合现实任务非线性特征的关键，通常情况下是**先验**的，需要研究人员精心设计得到，比如transformer和conv结构。

$X,Y$表示数据集中的输入和输出，参数$w$是神经网络参数。在**模型获取**阶段（训练），我们能够同时拿到数据集的输入和输出$X,Y$，并希望得到神经网络参数$w$；在**模型应用**阶段（推理），我们只拿到数据集中的$X$（输入数据），希望得到和真实世界任务尽可能相近的结果$Y$。

## 问题和梯度下降方法

在模型推理阶段中，由于我们已经得到了神经网络参数$w$，获得结果的过程是一个**前向计算**的过程，**按步骤**执行即可。

而在模型训练阶段，神经网络参数$w$是未知的，而函数$O(X)$的输出是已知的，由于神经网络参数$w$通常是高维向量，数据集$X,Y$同样具有一定规模，这相当于是在**近似求解一个高维方程**，这个问题通常是难解的，因此需要梯度下降，也就是**反向传播**过程。

将近似求解高维方程的问题转化为最小化loss函数的问题，令

![](/img/Gradient-Descent/loss.png)

在反向传播过程当中，由于数据集中的输入和输出都是已知的，即让该函数最小即可，根据优化理论，按照下式不断迭代，直到loss函数的变化小于某阈值即可。

![](/img/Gradient-Descent/gd.png)

## 一些难点

这里讨论一些梯度下降方法中确保准确率的一些难点。

- 关于数据集：由于数据集是通过实践获得的，只有**统计意义**上的理论保证。所以在每一轮迭代过程中如何选取数据集，如何保证数据集之间的偏差在一定范围内？

- 关于神经网络结构：由于神经网络的层状结构，存在梯度消失，梯度精度等问题
