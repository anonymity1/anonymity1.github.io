---
title: Octo INT8 training
date: 2021-09-29 21:40:55
tags: 论文阅读
---

Octo: INT8 Training with Loss-aware Compensation and Backward Quantization for Tiny On-device Learning (ATC 2021)

背景：系统，单设备，边缘+训练+量化。思路亮点：引入LAC+PRC。系统亮点：低依赖，只需python基本库。

<!-- more -->

边缘端高效模型训练实现边缘智能的关键环节。已有研究对边缘端数据收集和推理模型加速进行了深入探讨，量化（即将神经网络模型中的权重从FP32转为低精度格式数据，从而实现存储和计算消耗降低的方法）是数据中心实现高效模型推理和训练的一种方案。

## 问题

但是直接将已有的量化工作应用到轻量级设备模型训练的推理存在4个问题：

1.一些工作不适用模型训练；

2.一些工作不适用一般化网络结构，因为采用了极低比特设计；

3.不能够在训练阶段实现硬件层面实际加速（理论可以，实际不行）；

4.没有对训练阶段算法做充分优化，无法适应低资源设备。

## 设计挑战

解决以上问题需要设计一个原型系统，实现原型系统中遇到的挑战：

1. 如何充分利用硬件算力？（架构）

    > 将统一的8位量化引入卷积运算、仿射块、激活函数和梯度计算。数据量化涵盖前向和后向传递，因此可以加速整个迭代。

2. 保证精度？

    > 通过一起调整前向和后向传递的中间结果来保持模型的准确性。在前向传递中，提出了损失感知补偿(LAC)方法并设计了一个新的网络组件，称为补偿层，以填补量化张量算法引起的误差差距。补偿层内部的参数将根据网络损失进行优化，为了提高更新效率，引入了补偿参数的L2正则化项。在反向传播中，提出了参数化范围裁剪(PRC)来限制量化梯度的变换域。

3. 降低系统开销？尤其是内存和I/O?

    > 以INT8格式保留所有参数和中间导数。这有效地减少了峰值内存占用并节省了访问参数缓存的存储成本。此外，引入上一个问题中提到的LAC和PRC可能会产生额外的开销。将补偿项从使用FP32张量的更高次多项式转换为仅依赖于卷积层输出的仿射运算，这可以有效地限制补偿和裁剪的计算成本。

4. 通用性？

    > python+numpy编写

## 关于LAC和PRC层

待续
