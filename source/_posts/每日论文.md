---
title: 每日论文
date: 2021-12-09 10:32:55
tags: 论文阅读
---

> 1.只要把东西做出来，就会有和别人不一样的地方；

> 2.永远不要停止读论文。

<!-- more -->

## 12-09

Energy‑effective artificial internet‑of‑things application deployment in edge‑cloud systems (邓水光)

背景：云端边协同系统+任务卸载+DAG图建模。

亮点1：给出了一个具有DAG结构的AI任务例子，用来预测用户点击率的推荐算法DeepFM的结构。

![](/img/paper-for-everyday/Workload-of-DeepFM.png)

亮点2：具有DAG结构的AI任务卸载的架构图。

![](/img/paper-for-everyday/1.png)

TODO：关于DAG图的数学建模方法。

## 12-10

Online Social Welfare Maximization with Spatio-Temporal Resource Mesh for Serverless (赵银亮)、

无服务器时空资源网格的在线社会福利最大化

背景：无服务器+集群+时空网格资源分配+社会福利模型

亮点1：用时空资源网格表述集群中的流水线任务调度

![](/img/paper-for-everyday/Spatial-Temporal-Mesh.png)

亮点2：社会福利模型，即同时考虑供给侧和需求侧，对于集群来说，就是既要最优化QoS，又要使得集群的资源利用率最高。

TODO：时空资源网格式的数学描述，以及社会福利模型的对应算法和收敛界证明。

TODO：关于Serverless概念和在边缘计算中应用的探讨，Cloud programming simplified: A berkeley view on serverless computing

## 12-11

SDTP: Accelerating Wide-Area Data Analytics with Simultaneous Data Transfer and Processing (吴杰，TCC)

为了有效分析跨地域分布式数据流，云提供商在地理分布式站点（例如，数据中心、边缘集群和终端）之间实施数据并行作业，这些站点通常通过广域网链接互连。此文针对跨地域数据并行执行方法需要在每个阶段完成传输和计算时进行**等待产生的瓶颈**，提出了一种同时考虑网络带宽动态和作业并行性的同部数据传输和处理机制来加速广域数据分析。

背景：跨域数据分析+MapReduce作业流程

Mu: An Efficient, Fair and Responsive Serverless Framework for Resource-Constrained Edge Clouds

背景：系统，边缘云+无服务器架构。考虑可扩展性，集群内部资源利用率。

Edge computing for cyber-physical systems: A systematic mapping study emphasizing trustworthiness

背景：调研，边缘计算+安全（可信性）。关于安全性的论文研究内容的横向对比。

## 12-14

Cupy: A NumPy-Compatible Library for NVIDIA GPU Calculations (NIPs' 2017)

背景：

一个面向GPU计算，具有与numpy相似接口的python数学运算库开发和使用报告，用户可以自定义两类Cuda核操作，针对元素单体的操作和reduce操作。可以用做一些深度学习框架的后端。

## 12-15

multi-fiedlity information fusion with concatenated neural networks (arxiv' 21.04)

背景：工程领域问题模型构建。亮点：神经网络 + 物理模型先验知识。

计算建模已转向使用统计推理、深度学习和其他数据驱动的建模框架。尽管这种建模转变通过降低计算负担在设计优化和实时控制等许多应用中大有可为，但训练深度学习模型需要大量数据。这些大数据并不总是可用于解决科学问题，并导致数据驱动模型的泛化性较差。这种差距可以通过利用基于物理模型的信息来弥补，这篇论文利用研究问题的先验知识，提出了一种串联神经网络方法，以构建更量身定制、更有效和更高效的机器学习模型。在不失去其普遍性和模块化的情况下，这篇文章专注于层流和湍流边界层流预测模型的开发。

这项工作将自相似解和幂律速度分布（低保真模型）与通过级联神经网络从实验或计算流体动力学模拟（高保真模型）获得的噪声数据相结合。并说明了来自这些简化模型的知识如何减少与应用于边界层流预测问题的深度学习模型相关的不确定性。所提出的多保真信息融合框架产生了物理上一致的模型，这些模型试图比纯粹基于数据获得的数据驱动模型实现更好的泛化。虽然这篇文章针对与流体力学相关的问题展示了应用于特定问题的框架，但其工作流程和原则可用于许多普遍存在经验、分析或简化模型的科学问题。根据新的物理引导的机器学习原理的巨大需求，这项工作在广泛的基于物理的理论和数据驱动的建模范式之间架起了一座桥梁，并为将混合物理和机器学习建模方法用于下一代数字孪生铺平了道路。

Elf: Accelerate High-resolution Mobile Deep Vision with Content-aware Parallel Offloading (Mobicom 2021)

背景：边缘 + 视频流处理 + 部分任务卸载 + 内容感知。亮点：用LSTM预测需要卸载的图像区域，降低检测开销。

Towards General Deep Leakage in Federated Learning (arxiv' 21.10)

背景：联邦学习 + 攻击者原始数据重构。亮点：零标签恢复原始图像数据。

## 12-16

You Only Look Once: Unified, Real-Time Object Detection (arxiv' 15.06)

背景：CV + 目标检测。

Yolov1是针对目标检测算法fast-rcnn的两阶段检测-分类过程中速度过慢的痛点所提出来的目标检测算法。其全部由卷积层和全连接层构成，为了实现这样的一阶段检测+分类过程，Yolo将整个网络划分为SxS个网格。当有物体中心落在该网格中时，Yolo网络对其进行分类，其对应全连接层的输出包含三个部分，中心位置横纵坐标，检测框的长宽，位置置信概率IOU以及每一种分类对应的置信概率；当没有物体中心落在该网格中，对应的全连接层部分输出为0。

![](/img/paper-for-everyday/yolov1-structure.png)

这样的网络结构如何进行训练呢？Yolo将不同的预测指标loss加权求和作为总体的loss并进行训练。

![](/img/paper-for-everyday/yolov1-formula.png)

红框部分表示物体的坐标损失，因为大部分情况物体不一定处在指定网格中，因此其权重较高，为什么中心点没有取方根均方误差，而长宽取方根均方误差呢？这是因为当使用均方误差时，当loss相等，可能有两种情况：预测的比真实的大，或者预测的比真实的小。我们希望面积比值较大时，即预测值比真实值面积小时，其loss越大，因此需要用方根均方误差。黄色框部分是包含物体时和不包含物体时的置信概率loss，因为大部分情况网格里没有预测物体，因此我们希望不包含物体时权重较小，否则一样的权重会导致网络倾向于判断网格没有物体，因此该项权重设置较小。最后蓝色框里的loss表示分类误差。

## 12-18

Characterizing the I/O Pipeline in the Deployment of CNNs on Commercial Accelerators (ISPA' 2020)

商用AI加速器因其在深度神经网络（DNN）推理方面的高能效而越来越受欢迎。如何对它们进行基准测试仍然是热门的研究课题。现有的表征研究主要集中在硬件执行延迟上，而没有考虑在端到端推理中很重要的预处理或后处理以及拷入或拷出开销。受此启发，本文使用五阶段I/O管道对商业DNN加速器的端到端推理阶段进行建模，包括预处理、复制、硬件执行、复制和后处理阶段。我们进一步研究了每个阶段的涉及因素，并在三个不同的硬件平台上实现了整个I/O管道，涵盖了新兴的DNN加速器和传统的CPU和GPU。使用来自真实世界计算机视觉应用程序的六个DNN，我们深入研究DNN推理流水线并量化每个阶段对吞吐量的影响。本文实验结果证明了这种I/O管道的效果，并强调了端到端评估的必要性。

![](/img/paper-for-everyday/IO-pipeline.png)

## 12-20

Attention Is All You Need (NIPs' 2017)

背景：NLP + 自注意力机制。完全利用注意力机制实现自然语言处理系列任务。对于输入句子的相对位置关系使用一种正弦函数和余弦函数的先验编码方式。

![](/img/paper-for-everyday/transformer.png)

BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (arxiv' 2018)

背景：NLP + 自注意力编码结构堆叠。通过预训练得到句子的特征向量表示，然后通过微调的方法处理不同的下游自然语言处理任务。有两种预训练方式，一是句子间关系判断，这是有标签的数据集，二是句子内挖掉一个词，然后填空，这是无监督学习。

![](/img/paper-for-everyday/BERT.png)

除了token和position，type和mask也需要在训练时输入，用来表示句子内关系和句子间关系。

## 12-21

Adaptive Anomaly Detection for Internet of Things in Hierarchical Edge Computing: A Contextual-Bandit Approach (TOIT' 2021)

背景：分层式边缘网络结构 + 异常检测应用。

深度神经网络（DNN）的进步显着增强了物联网应用中异常数据的实时检测。然而，复杂度-准确度-延迟的困境依然存在：复杂的DNN模型提供了更高的准确度，但典型的物联网设备几乎无法承受计算负载，而将负载卸载到云端的补救措施会导致长时间的延迟。这篇文章通过提出具有分层边缘计算（HEC）的自适应异常检测方案来应对这一挑战，具体来说，首先构建多个复杂度不断增加的异常检测DNN模型，并将它们中的每一个与相应的HEC层相关联，然后设计了一个自适应模型选择方案，该方案被表述为上下文老虎机问题，并通过使用强化学习策略网络来解决。

异常检测与目标检测任务类似，是一个很大的分类，根据数据类型和异常类型的不同有多种检测方法和模型。

## 12-23

Learned TPU Cost Model for XLA Tensor Programs (NIPs' 2019)

谷歌公司希望开发一种成本模型，可以准确估计在张量处理单元（TPU）上运行的机器学习模型执行时间。编译器可以用这样的模型做出启发式决策，寻找特定程序的最佳配置，但是由于现代处理器的复杂性，构建准确的成本分析模型有挑战性。本文利用神经网络预测执行时间。

成本模型对于在编译器优化代码和手动优化程序很有用。例如，编译器后端LLVM的循环向量化器使用成本模型来计算最佳向量化因子和展开因子，而GCC的自动向量化器使用成本模型来决定何时应用循环剥离、循环版本控制、外循环矢量化和迭代内矢量化。更准确的成本模型通常会导致更好的优化决策。此外，可以在编译器的自动调谐器（autotuner）中使用成本模型，作为在真实硬件上生成和运行代码的更快替代方法。对于基于神经网络的推理模型来说，神经架构搜索也可以使用成本模型找到满足目标精度的最快模型，反之亦然。

目前在谷歌已经使用的自动调谐器XLA需要在真实机器上运行一次后得到真实执行时间，对于大型模型来说，跑一次测试太过耗时，因此需要比较准确的预估方法。考虑到现代处理器的设计复杂，手动设计成本设计模型需要考虑的因素过多，因此这篇文章采用的是将一个神经网络程序建模成图，根据图的特点利用神经网络推断其执行时间。

XLA是TensorFlow的编译器，适用于各种硬件目标，包括CPU、GPU、TPU和其他自定义加速器。一个TensorFlow图程序可以被翻译成一个XLA图程序，它是XLA编译器的输入。XLA程序由称为计算的基本块组成。每个计算都由称为计算图的有向无环图表示。计算图中的节点表示张量操作，将一个或多个输入张量处理为输出张量。边将一个节点的输出张量连接到另一个节点的输入张量。

XLA编译器执行诸如张量布局分配、操作融合和操作调度等优化。在优化之前，计算图中的节点是单个原始张量操作。经过优化，一个节点要么是单个原语操作，要么是融合操作（多个原语操作的融合）。在这篇文章中，将优化计算图中的节点称为核算子（kernel）。一个TPU一次执行一个核算子，分别在启动和终止时读取和写入主存；两个核算子之间的执行没有重叠。因此，计算图的执行时间是所有核算子的执行时间的总和，可能会有一些小例外。

![](/img/paper-for-everyday/XLA-graphsage.png)

上图描述了用于预测核算子执行时间的成本模型架构。从操作码，操作特征（张量维度，张量布局，算子特征），和邻接矩阵中捕获到的核算子链接预测执行时间。对于操作码和操作特征的编码采用线性方式，然后使用一层前馈层和graphsgae的方法生成图结构的嵌入表示（graph embedding）。

![](/img/paper-for-everyday/graph-embedding.png)

## 12-27

AI Accelerator Survey and Trends (arxiv' 2021)

在过去几年中，每月都会发布新的机器学习加速器，用于语音识别、视频对象检测、辅助驾驶和许多数据中心应用。这篇文章是对过去两年人工智能加速器和处理器的调查。此文收集和总结了目前已公开发布的具有峰值性能和功耗数字的商用加速器，并将性能和功率值绘制在散点图上，并再次讨论和分析该图上趋势的一些维度和观察结果。同时也编制了一份基准性能结果列表，并计算了与峰值性能相关的计算效率。

![](/img/paper-for-everyday/power-performance.png)

近年来，许多公司一直在报告其芯片、卡和系统的实际性能数据。他们一直在各种基准的背景下这样做，包括MLPerf。大多数基准测试结果都是用于推断的，其中度量是吞吐量即每秒图像/项目数。也有一些培训基准结果，其中指标是培训特定DNN模型的推理时间。此外，这篇文章也重点关注延迟限制内的每秒图像吞吐量，当前的国防和国家安全应用程序通常优先考虑吞吐量，因为来自传感器平台的图像/项目以一致的流收集。

![](/img/paper-for-everyday/utilization.png)

他的表格报告了主导推理（正向传递）计算的fused-multiply-add（FMA）操作的数量。然而，FMA操作只是所有计算和数据运动的近似值。上按每秒图像数（IPS）排序，底部有四个Google条目。谷歌报告了八个使用率最高的模型的平均计算效率结果。几乎所有的加速器都达到了20%以上的计算效率；通常在具有相当高运算强度的密集计算内核上实现10%的计算效率是一项挑战。考虑到这些具有宽数据传输路径的ML加速器的高度并行设计，可以合理预期，进一步的调整和优化将使用包括战略性数据布局和数据传输延迟隐藏在内的技术使这些加速器的利用率提高20%以上。另外，技术类型、精度或应用程序类别与计算利用率之间似乎没有相关性。
